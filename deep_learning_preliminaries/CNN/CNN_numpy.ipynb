{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codebase taken from:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/convolutional-neural-networks-from-the-ground-up-c67bb41454e1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Alescontrela/Numpy-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution(image, filt, bias, s=1):\n",
    "    '''\n",
    "    Confolves 'filt' over 'image' using stride 's'\n",
    "    '''\n",
    "    (n_f, n_c_f, f, _) = filt.shape # filter dimensions\n",
    "    n_c, in_dim, _ = image.shape # image dimensions\n",
    "    \n",
    "    out_dim = int((in_dim - f)/s)+1 # calculate output dimensions\n",
    "    \n",
    "    # ensure that the filter dimensions match the dimensions of the input image\n",
    "    assert n_c == n_c_f, \"Dimensions of filter must match dimensions of input image\"\n",
    "    \n",
    "    out = np.zeros((n_f,out_dim,out_dim)) # create the matrix to hold the values of the convolution operation\n",
    "    \n",
    "    # convolve each filter over the image\n",
    "    for curr_f in range(n_f):\n",
    "        curr_y = out_y = 0\n",
    "        # move filter vertically across the image\n",
    "        while curr_y + f <= in_dim:\n",
    "            curr_x = out_x = 0\n",
    "            # move filter horizontally across the image \n",
    "            while curr_x + f <= in_dim:\n",
    "                # perform the convolution operation and add the bias\n",
    "                out[curr_f, out_y, out_x] = np.sum(filt[curr_f]*image[:,curr_y:curr_y+f, curr_x:curr_x+f])+bias[curr_f]\n",
    "                curr_x += s\n",
    "                out_x += 1\n",
    "            curr_y += s\n",
    "            out_y += 1\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool(image, f=2, s=2):\n",
    "    '''\n",
    "    Downsample input 'image' using a kernel size of 'f' and a stride of 's'\n",
    "    '''\n",
    "    n_c, h_prev, w_prev = image.shape\n",
    "    \n",
    "    # calculate output dimensions after the maxpooling operation.\n",
    "    h = int((h_prev - f)/s)+1 \n",
    "    w = int((w_prev - f)/s)+1\n",
    "    \n",
    "    # create a matrix to hold the values of the maxpooling operation.\n",
    "    downsampled = np.zeros((n_c, h, w)) \n",
    "    \n",
    "    # slide the window over every part of the image using stride s. Take the maximum value at each step.\n",
    "    for i in range(n_c):\n",
    "        curr_y = out_y = 0\n",
    "        # slide the max pooling window vertically across the image\n",
    "        while curr_y + f <= h_prev:\n",
    "            curr_x = out_x = 0\n",
    "            # slide the max pooling window horizontally across the image\n",
    "            while curr_x + f <= w_prev:\n",
    "                # choose the maximum value within the window at each step and store it to the output matrix\n",
    "                downsampled[i, out_y, out_x] = np.max(image[i, curr_y:curr_y+f, curr_x:curr_x+f])\n",
    "                curr_x += s\n",
    "                out_x += 1\n",
    "            curr_y += s\n",
    "            out_y += 1\n",
    "    return downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(raw_preds):\n",
    "    '''\n",
    "    pass raw predictions through softmax activation function\n",
    "    '''\n",
    "    out = np.exp(raw_preds) # exponentiate vector of raw predictions\n",
    "    return out/np.sum(out) # divide the exponentiated vector by its sum. All values in the output sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoricalCrossEntropy(probs, label):\n",
    "    '''\n",
    "    calculate the categorical cross-entropy loss of the predictions\n",
    "    '''\n",
    "    # Multiply the desired output label by the log of the prediction, then sum all values in the vector\n",
    "    return -np.sum(label * np.log(probs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filename, num_images, IMAGE_WIDTH):\n",
    "    '''\n",
    "    Extract images by reading the file bytestream. Reshape the read values into a \n",
    "    3D matrix of dimensions [m, h, w], where m is the number of training examples.\n",
    "    '''\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(IMAGE_WIDTH * IMAGE_WIDTH * num_images)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = data.reshape(num_images, IMAGE_WIDTH*IMAGE_WIDTH)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(filename, num_images):\n",
    "    '''\n",
    "    Extract label into vector of integer values of dimensions [m, 1], where m is the number of images.\n",
    "    '''\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * num_images)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeFilter(size, scale = 1.0):\n",
    "    '''\n",
    "    Initialize filter using a normal distribution with and a \n",
    "    standard deviation inversely proportional the square root of the number of units\n",
    "    '''\n",
    "    stddev = scale/np.sqrt(np.prod(size))\n",
    "    return np.random.normal(loc = 0, scale = stddev, size = size)\n",
    "\n",
    "def initializeWeight(size):\n",
    "    '''\n",
    "    Initialize weights with a random normal distribution\n",
    "    '''\n",
    "    return np.random.standard_normal(size=size) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutionBackward(dconv_prev, conv_in, filt, s):\n",
    "    '''\n",
    "    Backpropagation through a convolutional layer. \n",
    "    '''\n",
    "    (n_f, n_c, f, _) = filt.shape\n",
    "    (_, orig_dim, _) = conv_in.shape\n",
    "    ## initialize derivatives\n",
    "    dout = np.zeros(conv_in.shape) \n",
    "    dfilt = np.zeros(filt.shape)\n",
    "    dbias = np.zeros((n_f,1))\n",
    "    for curr_f in range(n_f):\n",
    "        # loop through all filters\n",
    "        curr_y = out_y = 0\n",
    "        while curr_y + f <= orig_dim:\n",
    "            curr_x = out_x = 0\n",
    "            while curr_x + f <= orig_dim:\n",
    "                # loss gradient of filter (used to update the filter)\n",
    "                dfilt[curr_f] += dconv_prev[curr_f, out_y, out_x] * conv_in[:, curr_y:curr_y+f, curr_x:curr_x+f]\n",
    "                # loss gradient of the input to the convolution operation (conv1 in the case of this network)\n",
    "                dout[:, curr_y:curr_y+f, curr_x:curr_x+f] += dconv_prev[curr_f, out_y, out_x] * filt[curr_f] \n",
    "                curr_x += s\n",
    "                out_x += 1\n",
    "            curr_y += s\n",
    "            out_y += 1\n",
    "        # loss gradient of the bias\n",
    "        dbias[curr_f] = np.sum(dconv_prev[curr_f])\n",
    "    \n",
    "    return dout, dfilt, dbias\n",
    "\n",
    "def nanargmax(arr):\n",
    "    '''\n",
    "    return index of the largest non-nan value in the array. Output is an ordered pair tuple\n",
    "    '''\n",
    "    idx = np.nanargmax(arr)\n",
    "    idxs = np.unravel_index(idx, arr.shape)\n",
    "    return idxs \n",
    "\n",
    "def maxpoolBackward(dpool, orig, f, s):\n",
    "    '''\n",
    "    Backpropagation through a maxpooling layer. \n",
    "    The gradients are passed through the indices of greatest value in the\n",
    "    original maxpooling during the forward step.\n",
    "    '''\n",
    "    (n_c, orig_dim, _) = orig.shape\n",
    "    \n",
    "    dout = np.zeros(orig.shape)\n",
    "    \n",
    "    for curr_c in range(n_c):\n",
    "        curr_y = out_y = 0\n",
    "        while curr_y + f <= orig_dim:\n",
    "            curr_x = out_x = 0\n",
    "            while curr_x + f <= orig_dim:\n",
    "                # obtain index of largest value in input for current window\n",
    "                (a, b) = nanargmax(orig[curr_c, curr_y:curr_y+f, curr_x:curr_x+f])\n",
    "                dout[curr_c, curr_y+a, curr_x+b] = dpool[curr_c, out_y, out_x]\n",
    "                \n",
    "                curr_x += s\n",
    "                out_x += 1\n",
    "            curr_y += s\n",
    "            out_y += 1\n",
    "        \n",
    "    return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(image, label, params, conv_s, pool_f, pool_s):\n",
    "    \n",
    "    [f1, f2, w3, w4, b1, b2, b3, b4] = params \n",
    "    \n",
    "    ################################################\n",
    "    ############## Forward Operation ###############\n",
    "    ################################################\n",
    "    conv1 = convolution(image, f1, b1, conv_s) # convolution operation\n",
    "    conv1[conv1<=0] = 0 # pass through ReLU non-linearity\n",
    "    \n",
    "    conv2 = convolution(conv1, f2, b2, conv_s) # second convolution operation\n",
    "    conv2[conv2<=0] = 0 # pass through ReLU non-linearity\n",
    "    \n",
    "    pooled = maxpool(conv2, pool_f, pool_s) # maxpooling operation\n",
    "    \n",
    "    (nf2, dim2, _) = pooled.shape\n",
    "    fc = pooled.reshape((nf2 * dim2 * dim2, 1)) # flatten pooled layer\n",
    "    \n",
    "    z = w3.dot(fc) + b3 # first dense layer\n",
    "    z[z<=0] = 0 # pass through ReLU non-linearity\n",
    "    \n",
    "    out = w4.dot(z) + b4 # second dense layer\n",
    "     \n",
    "    probs = softmax(out) # predict class probabilities with the softmax activation function\n",
    "    \n",
    "    ################################################\n",
    "    #################### Loss ######################\n",
    "    ################################################\n",
    "    \n",
    "    loss = categoricalCrossEntropy(probs, label) # categorical cross-entropy loss\n",
    "        \n",
    "    ################################################\n",
    "    ############# Backward Operation ###############\n",
    "    ################################################\n",
    "    dout = probs - label # derivative of loss w.r.t. final dense layer output\n",
    "    dw4 = dout.dot(z.T) # loss gradient of final dense layer weights\n",
    "    db4 = np.sum(dout, axis = 1).reshape(b4.shape) # loss gradient of final dense layer biases\n",
    "    \n",
    "    dz = w4.T.dot(dout) # loss gradient of first dense layer outputs \n",
    "    dz[z<=0] = 0 # backpropagate through ReLU \n",
    "    dw3 = dz.dot(fc.T)\n",
    "    db3 = np.sum(dz, axis = 1).reshape(b3.shape)\n",
    "    \n",
    "    dfc = w3.T.dot(dz) # loss gradients of fully-connected layer (pooling layer)\n",
    "    dpool = dfc.reshape(pooled.shape) # reshape fully connected into dimensions of pooling layer\n",
    "    \n",
    "    dconv2 = maxpoolBackward(dpool, conv2, pool_f, pool_s) # backprop through the max-pooling layer(only neurons with highest activation in window get updated)\n",
    "    dconv2[conv2<=0] = 0 # backpropagate through ReLU\n",
    "    \n",
    "    dconv1, df2, db2 = convolutionBackward(dconv2, conv1, f2, conv_s) # backpropagate previous gradient through second convolutional layer.\n",
    "    dconv1[conv1<=0] = 0 # backpropagate through ReLU\n",
    "    \n",
    "    dimage, df1, db1 = convolutionBackward(dconv1, image, f1, conv_s) # backpropagate previous gradient through first convolutional layer.\n",
    "    \n",
    "    grads = [df1, df2, dw3, dw4, db1, db2, db3, db4] \n",
    "    \n",
    "    return grads, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adamGD(batch, num_classes, lr, dim, n_c, beta1, beta2, params, cost):\n",
    "    '''\n",
    "    update the parameters through Adam gradient descnet.\n",
    "    '''\n",
    "    [f1, f2, w3, w4, b1, b2, b3, b4] = params\n",
    "    \n",
    "    X = batch[:,0:-1] # get batch inputs\n",
    "    X = X.reshape(len(batch), n_c, dim, dim)\n",
    "    Y = batch[:,-1] # get batch labels\n",
    "    \n",
    "    cost_ = 0\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # initialize gradients and momentum,RMS params\n",
    "    df1 = np.zeros(f1.shape)\n",
    "    df2 = np.zeros(f2.shape)\n",
    "    dw3 = np.zeros(w3.shape)\n",
    "    dw4 = np.zeros(w4.shape)\n",
    "    db1 = np.zeros(b1.shape)\n",
    "    db2 = np.zeros(b2.shape)\n",
    "    db3 = np.zeros(b3.shape)\n",
    "    db4 = np.zeros(b4.shape)\n",
    "    \n",
    "    v1 = np.zeros(f1.shape)\n",
    "    v2 = np.zeros(f2.shape)\n",
    "    v3 = np.zeros(w3.shape)\n",
    "    v4 = np.zeros(w4.shape)\n",
    "    bv1 = np.zeros(b1.shape)\n",
    "    bv2 = np.zeros(b2.shape)\n",
    "    bv3 = np.zeros(b3.shape)\n",
    "    bv4 = np.zeros(b4.shape)\n",
    "    \n",
    "    s1 = np.zeros(f1.shape)\n",
    "    s2 = np.zeros(f2.shape)\n",
    "    s3 = np.zeros(w3.shape)\n",
    "    s4 = np.zeros(w4.shape)\n",
    "    bs1 = np.zeros(b1.shape)\n",
    "    bs2 = np.zeros(b2.shape)\n",
    "    bs3 = np.zeros(b3.shape)\n",
    "    bs4 = np.zeros(b4.shape)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        x = X[i]\n",
    "        y = np.eye(num_classes)[int(Y[i])].reshape(num_classes, 1) # convert label to one-hot\n",
    "        \n",
    "        # Collect Gradients for training example\n",
    "        grads, loss = conv(x, y, params, 1, 2, 2)\n",
    "        [df1_, df2_, dw3_, dw4_, db1_, db2_, db3_, db4_] = grads\n",
    "        \n",
    "        df1+=df1_\n",
    "        db1+=db1_\n",
    "        df2+=df2_\n",
    "        db2+=db2_\n",
    "        dw3+=dw3_\n",
    "        db3+=db3_\n",
    "        dw4+=dw4_\n",
    "        db4+=db4_\n",
    "\n",
    "        cost_+= loss\n",
    "\n",
    "    # Parameter Update  \n",
    "        \n",
    "    v1 = beta1*v1 + (1-beta1)*df1/batch_size # momentum update\n",
    "    s1 = beta2*s1 + (1-beta2)*(df1/batch_size)**2 # RMSProp update\n",
    "    f1 -= lr * v1/np.sqrt(s1+1e-7) # combine momentum and RMSProp to perform update with Adam\n",
    "    \n",
    "    bv1 = beta1*bv1 + (1-beta1)*db1/batch_size\n",
    "    bs1 = beta2*bs1 + (1-beta2)*(db1/batch_size)**2\n",
    "    b1 -= lr * bv1/np.sqrt(bs1+1e-7)\n",
    "   \n",
    "    v2 = beta1*v2 + (1-beta1)*df2/batch_size\n",
    "    s2 = beta2*s2 + (1-beta2)*(df2/batch_size)**2\n",
    "    f2 -= lr * v2/np.sqrt(s2+1e-7)\n",
    "                       \n",
    "    bv2 = beta1*bv2 + (1-beta1) * db2/batch_size\n",
    "    bs2 = beta2*bs2 + (1-beta2)*(db2/batch_size)**2\n",
    "    b2 -= lr * bv2/np.sqrt(bs2+1e-7)\n",
    "    \n",
    "    v3 = beta1*v3 + (1-beta1) * dw3/batch_size\n",
    "    s3 = beta2*s3 + (1-beta2)*(dw3/batch_size)**2\n",
    "    w3 -= lr * v3/np.sqrt(s3+1e-7)\n",
    "    \n",
    "    bv3 = beta1*bv3 + (1-beta1) * db3/batch_size\n",
    "    bs3 = beta2*bs3 + (1-beta2)*(db3/batch_size)**2\n",
    "    b3 -= lr * bv3/np.sqrt(bs3+1e-7)\n",
    "    \n",
    "    v4 = beta1*v4 + (1-beta1) * dw4/batch_size\n",
    "    s4 = beta2*s4 + (1-beta2)*(dw4/batch_size)**2\n",
    "    w4 -= lr * v4 / np.sqrt(s4+1e-7)\n",
    "    \n",
    "    bv4 = beta1*bv4 + (1-beta1)*db4/batch_size\n",
    "    bs4 = beta2*bs4 + (1-beta2)*(db4/batch_size)**2\n",
    "    b4 -= lr * bv4 / np.sqrt(bs4+1e-7)\n",
    "    \n",
    "\n",
    "    cost_ = cost_/batch_size\n",
    "    cost.append(cost_)\n",
    "\n",
    "    params = [f1, f2, w3, w4, b1, b2, b3, b4]\n",
    "    \n",
    "    return params, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_classes = 10, lr = 0.01, beta1 = 0.95, beta2 = 0.99, img_dim = 28, img_depth = 1, f = 5, num_filt1 = 8, num_filt2 = 8, batch_size = 32, num_epochs = 2, save_path = 'params.pkl'):\n",
    "\n",
    "    # Get training data\n",
    "    m =50000\n",
    "    X = extract_data('data/train-images-idx3-ubyte.gz', m, img_dim)\n",
    "    y_dash = extract_labels('data/train-labels-idx1-ubyte.gz', m).reshape(m,1)\n",
    "    X-= int(np.mean(X))\n",
    "    X/= int(np.std(X))\n",
    "    train_data = np.hstack((X,y_dash))\n",
    "\n",
    "    np.random.shuffle(train_data)\n",
    "\n",
    "    ## Initializing all the parameters\n",
    "    f1, f2, w3, w4 = (num_filt1 ,img_depth,f,f), (num_filt2 ,num_filt1,f,f), (128,800), (10, 128)\n",
    "    f1 = initializeFilter(f1)\n",
    "    f2 = initializeFilter(f2)\n",
    "    w3 = initializeWeight(w3)\n",
    "    w4 = initializeWeight(w4)\n",
    "\n",
    "    b1 = np.zeros((f1.shape[0],1))\n",
    "    b2 = np.zeros((f2.shape[0],1))\n",
    "    b3 = np.zeros((w3.shape[0],1))\n",
    "    b4 = np.zeros((w4.shape[0],1))\n",
    "\n",
    "    params = [f1, f2, w3, w4, b1, b2, b3, b4]\n",
    "\n",
    "    cost = []\n",
    "\n",
    "    print(\"LR:\"+str(lr)+\", Batch Size:\"+str(batch_size))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        np.random.shuffle(train_data)\n",
    "        batches = [train_data[k:k + batch_size] for k in range(0, train_data.shape[0], batch_size)]\n",
    "\n",
    "        t = tqdm(batches)\n",
    "        for x,batch in enumerate(t):\n",
    "            params, cost = adamGD(batch, num_classes, lr, img_dim, img_depth, beta1, beta2, params, cost)\n",
    "            t.set_description(\"Cost: %.2f\" % (cost[-1]))\n",
    "            \n",
    "\n",
    "    with open(save_path, 'wb') as file:\n",
    "        pickle.dump(params, file)\n",
    "        \n",
    "    return cost, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "LR:0.01, Batch Size:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cost: 0.67: 100%|██████████| 1563/1563 [9:51:19<00:00, 22.70s/it]  \n",
      "Cost: 0.00: 100%|██████████| 1563/1563 [12:04:12<00:00, 27.80s/it]  \n"
     ]
    }
   ],
   "source": [
    "cost, params = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image, f1, f2, w3, w4, b1, b2, b3, b4, conv_s = 1, pool_f = 2, pool_s = 2):\n",
    "    '''\n",
    "    Make predictions with trained filters/weights. \n",
    "    '''\n",
    "    conv1 = convolution(image, f1, b1, conv_s) # convolution operation\n",
    "    conv1[conv1<=0] = 0 #relu activation\n",
    "    \n",
    "    conv2 = convolution(conv1, f2, b2, conv_s) # second convolution operation\n",
    "    conv2[conv2<=0] = 0 # pass through ReLU non-linearity\n",
    "    \n",
    "    pooled = maxpool(conv2, pool_f, pool_s) # maxpooling operation\n",
    "    (nf2, dim2, _) = pooled.shape\n",
    "    fc = pooled.reshape((nf2 * dim2 * dim2, 1)) # flatten pooled layer\n",
    "    \n",
    "    z = w3.dot(fc) + b3 # first dense layer\n",
    "    z[z<=0] = 0 # pass through ReLU non-linearity\n",
    "    \n",
    "    out = w4.dot(z) + b4 # second dense layer\n",
    "    probs = softmax(out) # predict class probabilities with the softmax activation function\n",
    "    \n",
    "    return np.argmax(probs), np.max(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_ = pickle.load(open('params.pkl', 'rb'))\n",
    "[f1, f2, w3, w4, b1, b2, b3, b4] = params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing accuracy over test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Acc:98.26%: 100%|██████████| 10000/10000 [55:43<00:00,  2.99it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 98.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get test data\n",
    "m =10000\n",
    "X = extract_data('data/t10k-images-idx3-ubyte.gz', m, 28)\n",
    "y_dash = extract_labels('data/t10k-labels-idx1-ubyte.gz', m).reshape(m,1)\n",
    "# Normalize the data\n",
    "X-= int(np.mean(X)) # subtract mean\n",
    "X/= int(np.std(X)) # divide by standard deviation\n",
    "test_data = np.hstack((X,y_dash))\n",
    "\n",
    "X = test_data[:,0:-1]\n",
    "X = X.reshape(len(test_data), 1, 28, 28)\n",
    "y = test_data[:,-1]\n",
    "\n",
    "corr = 0\n",
    "digit_count = [0 for i in range(10)]\n",
    "digit_correct = [0 for i in range(10)]\n",
    "\n",
    "print()\n",
    "print(\"Computing accuracy over test set:\")\n",
    "\n",
    "t = tqdm(range(len(X)), leave=True)\n",
    "\n",
    "for i in t:\n",
    "    x = X[i]\n",
    "    pred, prob = predict(x, f1, f2, w3, w4, b1, b2, b3, b4)\n",
    "    digit_count[int(y[i])]+=1\n",
    "    if pred==y[i]:\n",
    "        corr+=1\n",
    "        digit_correct[pred]+=1\n",
    "\n",
    "    t.set_description(\"Acc:%0.2f%%\" % (float(corr/(i+1))*100))\n",
    "\n",
    "print(\"Overall Accuracy: %.2f\" % (float(corr/len(test_data)*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
