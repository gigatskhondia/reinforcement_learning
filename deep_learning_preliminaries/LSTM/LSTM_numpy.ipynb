{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/back-to-basics-deriving-back-propagation-on-simple-rnn-lstm-feat-aidan-gomez-c7f286ba973d\n",
    "\n",
    "https://christinakouridi.blog/2019/06/20/vanilla-lstm-numpy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codebase taken from:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.varunajayasiri.com/numpy_lstm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('Huxley_-_Brave_New_World.txt', 'r', encoding=\"ISO-8859-1\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data and calculate indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 377900 characters, 81 unique\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, X_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_size = 100 # Size of the hidden layer\n",
    "T_steps = 25 # Number of time steps (length of the sequence) used for training\n",
    "learning_rate = 1e-1 # Learning rate\n",
    "weight_sd = 0.1 # Standard deviation of weights for initialization\n",
    "z_size = H_size + X_size # Size of concatenate(H, X) vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions and Derivatives\n",
    "\n",
    "#### Sigmoid\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}}\\\\\n",
    "\\frac{d\\sigma(x)}{dx} &= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align}\n",
    "\n",
    "#### Tanh\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d\\text{tanh}(x)}{dx} &= 1 - \\text{tanh}^2(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Param:\n",
    "    def __init__(self, name, value):\n",
    "        self.name = name\n",
    "        self.v = value #parameter value\n",
    "        self.d = np.zeros_like(value) #derivative\n",
    "        self.m = np.zeros_like(value) #momentum for AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use random weights with normal distribution (`0`, `weight_sd`) for $tanh$ activation function and (`0.5`, `weight_sd`) for $sigmoid$ activation function.\n",
    "\n",
    "Biases are initialized to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        self.W_f = Param('W_f', \n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_f = Param('b_f',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_i = Param('W_i',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_i = Param('b_i',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_C = Param('W_C',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd)\n",
    "        self.b_C = Param('b_C',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_o = Param('W_o',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_o = Param('b_o',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        #For final layer to predict the next character\n",
    "        self.W_v = Param('W_v',\n",
    "                         np.random.randn(X_size, H_size) * weight_sd)\n",
    "        self.b_v = Param('b_v',\n",
    "                         np.zeros((X_size, 1)))\n",
    "        \n",
    "    def all(self):\n",
    "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
    "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
    "        \n",
    "parameters = Parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "![LSTM](http://blog.varunajayasiri.com/ml/lstm.svg)\n",
    "\n",
    "*Operation $z$ is the concatenation of $x$ and $h_{t-1}$*\n",
    "\n",
    "#### Concatenation of $h_{t-1}$ and $x_t$\n",
    "\\begin{align}\n",
    "z & = [h_{t-1}, x_t] \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### LSTM functions\n",
    "\\begin{align}\n",
    "f_t & = \\sigma(W_f \\cdot z + b_f) \\\\\n",
    "i_t & = \\sigma(W_i \\cdot z + b_i) \\\\\n",
    "\\bar{C}_t & = tanh(W_C \\cdot z + b_C) \\\\\n",
    "C_t & = f_t * C_{t-1} + i_t * \\bar{C}_t \\\\\n",
    "o_t & = \\sigma(W_o \\cdot z + b_t) \\\\\n",
    "h_t &= o_t * tanh(C_t) \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Logits\n",
    "\\begin{align}\n",
    "v_t &= W_v \\cdot h_t + b_v \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Softmax\n",
    "\\begin{align}\n",
    "\\hat{y_t} &= \\text{softmax}(v_t)\n",
    "\\end{align}\n",
    "\n",
    "$\\hat{y_t}$ is `y` in code and $y_t$ is `targets`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, h_prev, C_prev, p = parameters):\n",
    "    assert x.shape == (X_size, 1)\n",
    "    assert h_prev.shape == (H_size, 1)\n",
    "    assert C_prev.shape == (H_size, 1)\n",
    "    \n",
    "    z = np.row_stack((h_prev, x))\n",
    "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
    "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
    "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
    "\n",
    "    C = f * C_prev + i * C_bar\n",
    "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
    "    h = o * tanh(C)\n",
    "\n",
    "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
    "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
    "\n",
    "    return z, f, i, C_bar, C, o, h, v, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "#### Loss\n",
    "\n",
    "\\begin{align}\n",
    "L_k &= -\\sum_{t=k}^T\\sum_j y_{t,j} log \\hat{y_{t,j}} \\\\\n",
    "L &= L_1 \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Gradients\n",
    "\n",
    "\\begin{align}\n",
    "dv_t &= \\hat{y_t} - y_t \\\\\n",
    "dh_t &= dh'_t + W_y^T \\cdot dv_t \\\\\n",
    "do_t &= dh_t * \\text{tanh}(C_t) \\\\\n",
    "dC_t &= dC'_t + dh_t * o_t * (1 - \\text{tanh}^2(C_t))\\\\\n",
    "d\\bar{C}_t &= dC_t * i_t \\\\\n",
    "di_t &= dC_t * \\bar{C}_t \\\\\n",
    "df_t &= dC_t * C_{t-1} \\\\\n",
    "\\\\\n",
    "df'_t &= f_t * (1 - f_t) * df_t \\\\\n",
    "di'_t &= i_t * (1 - i_t) * di_t \\\\\n",
    "d\\bar{C}'_{t-1} &= (1 - \\bar{C}_t^2) * d\\bar{C}_t \\\\\n",
    "do'_t &= o_t * (1 - o_t) * do_t \\\\\n",
    "dz_t &= W_f^T \\cdot df'_t \\\\\n",
    "     &+ W_i^T \\cdot di_t \\\\\n",
    "     &+ W_C^T \\cdot d\\bar{C}_t \\\\\n",
    "     &+ W_o^T \\cdot do_t \\\\\n",
    "\\\\\n",
    "[dh'_{t-1}, dx_t] &= dz_t \\\\\n",
    "dC'_t &= f_t * dC_t\n",
    "\\end{align}\n",
    "\n",
    "* $dC'_t = \\frac{\\partial L_{t+1}}{\\partial C_t}$ and $dh'_t = \\frac{\\partial L_{t+1}}{\\partial h_t}$\n",
    "* $dC_t = \\frac{\\partial L}{\\partial C_t} = \\frac{\\partial L_t}{\\partial C_t}$ and $dh_t = \\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L_{t}}{\\partial h_t}$\n",
    "* All other derivatives are of $L$\n",
    "* `target` is target character index $y_t$\n",
    "* `dh_next` is $dh'_{t}$ (size H x 1)\n",
    "* `dC_next` is $dC'_{t}$ (size H x 1)\n",
    "* `C_prev` is $C_{t-1}$ (size H x 1)\n",
    "* $df'_t$, $di'_t$, $d\\bar{C}'_t$, and $do'_t$ are *also* assigned to `df`, `di`, `dC_bar`, and `do` in the **code**.\n",
    "* *Returns* $dh_t$ and $dC_t$\n",
    "\n",
    "#### Model parameter gradients\n",
    "\n",
    "\\begin{align}\n",
    "dW_v &= dv_t \\cdot h_t^T \\\\\n",
    "db_v &= dv_t \\\\\n",
    "\\\\\n",
    "dW_f &= df'_t \\cdot z^T \\\\\n",
    "db_f &= df'_t \\\\\n",
    "\\\\\n",
    "dW_i &= di'_t \\cdot z^T \\\\\n",
    "db_i &= di'_t \\\\\n",
    "\\\\\n",
    "dW_C &= d\\bar{C}'_t \\cdot z^T \\\\\n",
    "db_C &= d\\bar{C}'_t \\\\\n",
    "\\\\\n",
    "dW_o &= do'_t \\cdot z^T \\\\\n",
    "db_o &= do'_t \\\\\n",
    "\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(target, dh_next, dC_next, C_prev,\n",
    "             z, f, i, C_bar, C, o, h, v, y,\n",
    "             p = parameters):\n",
    "    \n",
    "    assert z.shape == (X_size + H_size, 1)\n",
    "    assert v.shape == (X_size, 1)\n",
    "    assert y.shape == (X_size, 1)\n",
    "    \n",
    "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
    "        assert param.shape == (H_size, 1)\n",
    "        \n",
    "    dv = np.copy(y)\n",
    "    dv[target] -= 1\n",
    "\n",
    "    p.W_v.d += np.dot(dv, h.T)\n",
    "    p.b_v.d += dv\n",
    "\n",
    "    dh = np.dot(p.W_v.v.T, dv)        \n",
    "    dh += dh_next\n",
    "    do = dh * tanh(C)\n",
    "    do = dsigmoid(o) * do\n",
    "    p.W_o.d += np.dot(do, z.T)\n",
    "    p.b_o.d += do\n",
    "\n",
    "    dC = np.copy(dC_next)\n",
    "    dC += dh * o * dtanh(tanh(C))\n",
    "    dC_bar = dC * i\n",
    "    dC_bar = dtanh(C_bar) * dC_bar\n",
    "    p.W_C.d += np.dot(dC_bar, z.T)\n",
    "    p.b_C.d += dC_bar\n",
    "\n",
    "    di = dC * C_bar\n",
    "    di = dsigmoid(i) * di\n",
    "    p.W_i.d += np.dot(di, z.T)\n",
    "    p.b_i.d += di\n",
    "\n",
    "    df = dC * C_prev\n",
    "    df = dsigmoid(f) * df\n",
    "    p.W_f.d += np.dot(df, z.T)\n",
    "    p.b_f.d += df\n",
    "\n",
    "    dz = (np.dot(p.W_f.v.T, df)\n",
    "         + np.dot(p.W_i.v.T, di)\n",
    "         + np.dot(p.W_C.v.T, dC_bar)\n",
    "         + np.dot(p.W_o.v.T, do))\n",
    "    dh_prev = dz[:H_size, :]\n",
    "    dC_prev = f * dC\n",
    "    \n",
    "    return dh_prev, dC_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear gradients before each backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.d.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip gradients to mitigate exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        np.clip(p.d, -1, 1, out=p.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n",
    "\n",
    "* `input`, `target` are list of integers, with character indexes.\n",
    "* `h_prev` is the array of initial `h` at $h_{-1}$ (size H x 1)\n",
    "* `C_prev` is the array of initial `C` at $C_{-1}$ (size H x 1)\n",
    "* *Returns* loss, final $h_T$ and $C_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward(inputs, targets, h_prev, C_prev):\n",
    "    global paramters\n",
    "    \n",
    "    # To store the values for each time step\n",
    "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
    "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
    "    v_s, y_s =  {}, {}\n",
    "    \n",
    "    # Values at t - 1\n",
    "    h_s[-1] = np.copy(h_prev)\n",
    "    C_s[-1] = np.copy(C_prev)\n",
    "    \n",
    "    loss = 0\n",
    "    # Loop through time steps\n",
    "    assert len(inputs) == T_steps\n",
    "    for t in range(len(inputs)):\n",
    "        x_s[t] = np.zeros((X_size, 1))\n",
    "        x_s[t][inputs[t]] = 1 # Input character\n",
    "        \n",
    "        (z_s[t], f_s[t], i_s[t],\n",
    "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
    "        v_s[t], y_s[t]) = \\\n",
    "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
    "            \n",
    "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
    "        \n",
    "    clear_gradients()\n",
    "\n",
    "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
    "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Backward pass\n",
    "        dh_next, dC_next = \\\n",
    "            backward(target = targets[t], dh_next = dh_next,\n",
    "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
    "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
    "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
    "                     y = y_s[t])\n",
    "\n",
    "    clip_gradients()\n",
    "        \n",
    "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
    "    x = np.zeros((X_size, 1))\n",
    "    x[first_char_idx] = 1\n",
    "\n",
    "    h = h_prev\n",
    "    C = C_prev\n",
    "\n",
    "    indexes = []\n",
    "    \n",
    "    for t in range(sentence_length):\n",
    "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
    "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
    "        x = np.zeros((X_size, 1))\n",
    "        x[idx] = 1\n",
    "        indexes.append(idx)\n",
    "\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (Adagrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the graph and display a sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status(inputs, h_prev, C_prev):\n",
    "    #initialized later\n",
    "    global plot_iter, plot_loss\n",
    "    global smooth_loss\n",
    "    \n",
    "    # Get predictions for 200 letters with current model\n",
    "\n",
    "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
    "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
    "\n",
    "    # Clear and plot\n",
    "    plt.plot(plot_iter, plot_loss)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()\n",
    "\n",
    "    #Print prediction and loss\n",
    "    print(\"----\\n %s \\n----\" % (txt, ))\n",
    "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update parameters\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_i &= \\theta_i - \\eta\\frac{d\\theta_i}{\\sum dw_{\\tau}^2} \\\\\n",
    "d\\theta_i &= \\frac{\\partial L}{\\partial \\theta_i}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_paramters(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.m += p.d * p.d # Calculate sum of gradients\n",
    "        #print(learning_rate * dparam)\n",
    "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delay the keyboard interrupt to prevent the training \n",
    "from stopping in the middle of an iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "class DelayedKeyboardInterrupt(object):\n",
    "    def __enter__(self):\n",
    "        self.signal_received = False\n",
    "        self.old_handler = signal.signal(signal.SIGINT, self.handler)\n",
    "\n",
    "    def handler(self, sig, frame):\n",
    "        self.signal_received = (sig, frame)\n",
    "        print('SIGINT received. Delaying KeyboardInterrupt.')\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        signal.signal(signal.SIGINT, self.old_handler)\n",
    "        if self.signal_received:\n",
    "            self.old_handler(*self.signal_received)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential average of loss\n",
    "# Initialize to a error of a random model\n",
    "smooth_loss = -np.log(1.0 / X_size) * T_steps\n",
    "\n",
    "iteration, pointer = 0, 0\n",
    "\n",
    "# For the graph\n",
    "plot_iter = np.zeros((0))\n",
    "plot_loss = np.zeros((0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD0CAYAAABtjRZ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de1wU9foH8M+woFwWUMQUQgSUMiA02tQ6q2mJGKbWORqaPz2plVmu2cUEBNQgL93LS1nZ6Ry1vGRlpeYtifACRSoCqamgCYggXmDluju/P5a9sbNXZmGHfd6vV69kdmb22dnZZ77zvQ3DsiwLQgghguTS0QEQQgixHSVxQggRMErihBAiYJTECSFEwCiJE0KIgLm21xvV19ejoKAAPXv2hEgkaq+3JYQQQVMoFKisrERUVBTc3d0NXm+3JF5QUICpU6e219sRQkinsmnTJkgkEoPl7ZbEe/bsqQmkd+/e7fW2hBAiaJcvX8bUqVM1ObS1dkvi6iqU3r17IygoqL3elhBCOgVj1dDUsEkIIQJGSZwQQgSMkjghhAgYJXFCCBEwSuKEECJglMQJIUTABJHEr9TUIyRxJw78WdHRoRBCiEMRRBIvLL0JANhw9EIHR0IIIY5FEEmcEEIIN0rihBAiYGaH3SsUCqSkpKC4uBgMw2Dp0qVobm7G7NmzERISAgCYMmUK4uPjsXr1amRmZsLV1RXJycmIjo62d/yEEOLUzCbxgwcPAgA2b96MnJwcvPfee3jooYcwY8YMzJw5U7NeYWEhcnNzsW3bNpSXl0Mmk2H79u32i5wQQoj5JD5q1CiMGDECAFBWVgYfHx8UFBSguLgYBw4cQN++fZGcnIy8vDxIpVIwDIPAwEAoFApUV1fDz8/P3p+BEEKclkWzGLq6umLhwoXYt28fPvzwQ1RUVGDSpEmIiorCRx99hDVr1sDb2xvdunXTbOPl5YWamhpK4oQQYkcWN2yuXLkSe/bsQWpqKqRSKaKiogAAsbGxKCoqglgshlwu16wvl8vh7e3Nf8SEEEI0zCbx7777DuvWrQMAeHh4gGEYzJ07F/n5+QCAI0eOIDIyEjExMcjOzoZSqURZWRmUSiVvpXAWLC/7IYSQzsZsdcro0aORlJSEqVOnorm5GcnJyQgICEB6ejrc3Nzg7++P9PR0iMViSCQSJCQkQKlUIi0tjfdgGd73SAghwmY2iXt6euKDDz4wWL5582aDZTKZDDKZjJ/ICCGEmCWIwT4s1aYQQggnQSRxNYahChVCCNElqCROCCFEHyVxQggRMErihBAiYIJI4tSwSQgh3ASRxNWoWZMQQvQJKokTQgjRR0mcEEIEjJI4IYQIGCVxQggRMErihBAiYIJI4tTDkBBCuAkjibd0FKepUwghRJ8gkrgWZXFCCNElsCROCCFEFyVxQggRMErihBAiYJTECSFEwCiJE0KIgJl9ULJCoUBKSgqKi4vBMAyWLl2Krl27IjExEQzDIDw8HIsXL4aLiwtWr16NzMxMuLq6Ijk5GdHR0e3xGQghxGmZTeIHDx4EoHq6fU5ODt577z2wLIv58+djyJAhSEtLw4EDBxAYGIjc3Fxs27YN5eXlkMlk2L59u90/ACGEODOzSXzUqFEYMWIEAKCsrAw+Pj44fPgwBg8eDAAYPnw4Dh06hNDQUEilUjAMg8DAQCgUClRXV8PPz6/NQdKITUII4WZRnbirqysWLlyI9PR0jBs3DizLap487+XlhZqaGtTW1kIsFmu2US/nE43YJIQQfRY3bK5cuRJ79uxBamoqGhoaNMvlcjl8fHwgFoshl8v1lnt7e/MSJD2ejRBCuJlN4t999x3WrVsHAPDw8ADDMIiKikJOTg4AICsrCxKJBDExMcjOzoZSqURZWRmUSiUvVSm6qCBOCCH6zNaJjx49GklJSZg6dSqam5uRnJyMfv36ITU1Fe+++y7CwsIQFxcHkUgEiUSChIQEKJVKpKWltUf8hBDi1MwmcU9PT3zwwQcGyzdu3GiwTCaTQSaT8RMZIYQQs2iwDyGECBglcUIIETCBJHHqnkIIIVwEksRVqJ84IYToE1QSJ4QQoo+SOCGECBglcUIIETBK4oQQImCCSOI0dwohhHATRBJXY2j2FEII0SOoJE4IIUSfIJI41aYQQgg3QSRxNRrsQwgh+gSVxAkhhOijJE4IIQJGSZwQQgSMkjghhAgYJXFCCBEwSuKEECJggkjiNOyeEEK4CSOJtwz3oX7ihBCiz+TT7puampCcnIzS0lI0NjZizpw5CAgIwOzZsxESEgIAmDJlCuLj47F69WpkZmbC1dUVycnJiI6O5j1YmjuFEEL0mUzi33//Pbp164a33noL169fx2OPPYYXXngBM2bMwMyZMzXrFRYWIjc3F9u2bUN5eTlkMhm2b99u9+AJIcTZmUziY8aMQVxcHACAZVmIRCIUFBSguLgYBw4cQN++fZGcnIy8vDxIpVIwDIPAwEAoFApUV1fDz8+vXT4EIYQ4K5NJ3MvLCwBQW1uLefPmYf78+WhsbMSkSZMQFRWFjz76CGvWrIG3tze6deumt11NTQ0lcUIIsTOzDZvl5eWYPn06JkyYgHHjxiE2NhZRUVEAgNjYWBQVFUEsFkMul2u2kcvl8Pb2tl/UhBBCAJhJ4lVVVZg5cyYWLFiAiRMnAgBmzZqF/Px8AMCRI0cQGRmJmJgYZGdnQ6lUoqysDEqlkkrhhBDSDkxWp3z88ce4efMm1q5di7Vr1wIAEhMTsWzZMri5ucHf3x/p6ekQi8WQSCRISEiAUqlEWlpauwRPCCHOzmQST0lJQUpKisHyzZs3GyyTyWSQyWT8RaaDBvsQQgg3QQz20aBu4oQQokdYSZwQQogeSuKEECJggkjiVCVOCCHcBJHE1ahKnBBC9AkqiRNCCNFHSZwQQgRMEEmcpY7ihBDCSRBJXI2hp0IQQogeQSVxQggh+iiJE0KIgFESJ4QQAaMkTgghAkZJnBBCBIySOCGECJigkjh1MCSEEH2CSOLqsT7UTZwQQvQJIokTQgjhRkmcEEIEjJI4IYQImMkHJTc1NSE5ORmlpaVobGzEnDlz0L9/fyQmJoJhGISHh2Px4sVwcXHB6tWrkZmZCVdXVyQnJyM6Orq9PgMhhDgtk0n8+++/R7du3fDWW2/h+vXreOyxxzBgwADMnz8fQ4YMQVpaGg4cOIDAwEDk5uZi27ZtKC8vh0wmw/bt29vrMxBCiNMyWZ0yZswYvPjiiwBU08GKRCIUFhZi8ODBAIDhw4fj8OHDyMvLg1QqBcMwCAwMhEKhQHV1Ne/B7jheRtPSEkKIDpNJ3MvLC2KxGLW1tZg3bx7mz58PlmU1U8J6eXmhpqYGtbW1EIvFetvV1NTYJeCfCi7bZb+EECJEZhs2y8vLMX36dEyYMAHjxo2Di4t2E7lcDh8fH4jFYsjlcr3l3t7evAXJ6jwq+WZ9E2/7JYQQoTOZxKuqqjBz5kwsWLAAEydOBABEREQgJycHAJCVlQWJRIKYmBhkZ2dDqVSirKwMSqUSfn5+9o+eEEKcnMmGzY8//hg3b97E2rVrsXbtWgDAokWLkJGRgXfffRdhYWGIi4uDSCSCRCJBQkIClEol0tLS7BYwVYkTQoiWySSekpKClJQUg+UbN240WCaTySCTyfiLTAclbkII4UaDfQghRMAoiRNCiIAJLolTzQohhGgJLolzOVNRg5DEnTh4+kpHh0IIIe1KEEncXMPm7yXXAAB7C2kgECHEuQgiiRNCCOEmuCTOVSpnqaacEOKkBJfETaPntxEidFM+OYqkb/I7OgzB6BRJnJ7BSYhwLPr2JBbvKDD6+pHzV/FV7t/tGJGwdY4k3vJ/yuGEOJb0H4sQkrhTb9mmnIv475ELHRRR5yOIJM5V413fpMDKn06hrlHR7vGQjnHjVhNO/H29o8MgVlifXdzRIXR6JudOcWT/O1KCjzLPoaurC3p4denocEg7mLr+KApKb6JkxdiODoUQhyGIkjiXxmYlAKBJodQsozrxzq2g9GZHh0CIwxFEEtd9JBtXd0JtnThlcUKIcxFEEtdVeq3OYBn1TiGEOCvBJfFbHA2Z9PBk4qwam5UoqZKbX5Ho+b2kGiGJO3H5Rn1Hh9Jmgkji9c3aem9TCZsK4sJR16jAku8LIW9o7uhQBG3x94UY8XYmrtY2dHQogrLhqKqL49HzVzs4krYTRBK/pfNDpzJ35/D5oWJ8cbgEn2Sd7+hQBO3Q2SoAQK2TXQzTdhQY9D+3Rme6eRdEEtelbHX0WVanYZMqxQVDoVR9a62/T0LMKSi9gf/xNFioM6QMQSRx3QOtbcRkDJYR5/bHxWuY+cVvaNbpduosnOk38Oiq7I4OwaFYlMRPnDiBadOmAQCKioowbNgwTJs2DdOmTcOuXbsAAKtXr8bEiRMxefJk5OfzO3nNVXmj5t+tz9UvDpfg9R+LeH0/IkyyL4/h51NXUN4JGqss1RlKkh2hM13zzI7Y/PTTT/H999/Dw8MDAFBYWIgZM2Zg5syZmnUKCwuRm5uLbdu2oby8HDKZDNu3b+ctyMoabaNN6xKHbm8VOqEJH/6uvoWe3l3h7ibq6FAIMctsSTw4OBirVq3S/F1QUIDMzExMnToVycnJqK2tRV5eHqRSKRiGQWBgIBQKBaqrq+0Usm3XUIWSpa6ITiJtRwFuNdrW0NesUGLYmwch++oYz1ERYh9mk3hcXBxcXbUF9ujoaLz22mvYtGkT+vTpgzVr1qC2thZisVizjpeXF2pqaviLUif3Kk1UdxobsXn5Rj36Je+i6S07OfVF+uDpSvznUIlN+1C07OOX05V8hdUuqHjivKxu2IyNjUVUVJTm30VFRRCLxZDLtQMO5HI5vL29+YuSA1ep2lh1SnHLYIgdx0vtGRJxIM5y10U1iMTqJD5r1ixNw+WRI0cQGRmJmJgYZGdnQ6lUoqysDEqlEn5+fvxFqds7xYYyhzM+vq2xWQnZV8ecajSf7rfsJDmc6Dh56YbTXLx1WT0V7ZIlS5Ceng43Nzf4+/sjPT0dYrEYEokECQkJUCqVSEtLs0esAAAlRxdDNXOlEmdq+PytpBo/nCjD1doGfPnM0I4Op0N9sP8v3B3kg4cG9DK7rhPmgE4h60wlpn+ei9cnRGL6/SEdHU67siiJBwUFYevWrQCAyMhIbN682WAdmUwGmUzGb3RqOj8sm35k9MN0Crrnhu5X/t7+MwBg3TzkTnTB7wwuVN8CAJy+zGNbnEAIYrCPrpv1TWho5n6aj7GSNl9T1f6YX4bNuRfbtI/25kwlS2esNiNEcEl8X1EFpq3P5XzN2LB7Y1PVLt/1J2LS91n83nO/PIbEb05avH5HcvaCpDNdvIiWM37tgkviAJBbbF0fdHUJrXUSX5d1HtU6o0E7I2cqnTpz4nbGBj1dzlxoEUQSt/T0NNuw6UxftQA/as75q5j+ea5mcixr6fVOaePFSyiHjyZ9c2wsy2oeJWkvgkjibaUupGSfrUJh2Y2ODaadCamAJvvqGLLOVKLKxrmxhfRZiX042jnwxeES3JGyG1dq7DefjyCSuMVlDQtWHPuhc8yA5lR3HRwc7cfcGsuyeH//GVy6douf/fGyF+Fy1BuS746XAeB+rCRfBJHELa9OMdKwyV8ogiOkz+4IsbZX8j9fJcf7+//C7A15bdqPg+auDuQIZ1H7EkYSb+MvyxkbfRy1ZGIJ20NnOf5lYwx2Pn7qev8GO9eXks5PGEncwvXM9RN3SgL68JprrY0JVIjXagFfax2Ugx3RdjgpBZHELe2sYPTrE+CP2zm1dAW18Yeo9zU7eEY3NnaBtJVjfu/27EUkiCRuaXUI/SCEzRHybnv1q2fbeMFqL5KM/Ri/2vE7Azj6cbRnla7VE2AJkTMNeFGz9ynNsixYFnBx4f+dbL0Y6/5Q2lwnbucj6AgXLEtU1TbY3OWTtA9hlMTbur1AfjD2YK8L2IrdpxCWvIvXhxK3sUpc75PaOmCovfBdneLM57guRz0OTl+dYmkecvRbKkvtzC/HnsLLbdqHvUfyfXG4BADQzGOyVJek+Yh9W96lNu/Dnni7uHaOU77NnLkqtVNVpxjtneKgV2djXvjyDwBWTp1qhL0/O5/75zPUm3VNPO7NfmjYfOfWHqlHGCVxCxn7OQgsh/PC3rnBnvu3uTrFyHzijkhTncLfHnnbk5AJrcDGB0Ek8bbeejrjYB81e39yPuvc21pPLMTvua0XQyrHO7b2+H6EkcSF99vscPY+eezR/qCpE+ejn7iD4+ucFtJnJvYhiCRucTc2Yw+F4DEWoRFS6dTSSIX0mYxR38G48FYv5dxlcms/fXudQ1Qn3iIiwMdgGde5b7ROXPi/eau1V3uZrcfW5HZ89DG0dRftdK7wP2LTCU9yHrRXw7I930UQSZzL1xxdyIx/H3SC880u534bvyY+v2V7/7b5itW5y9+GnHFgn0VJ/MSJE5g2bRoA4MKFC5gyZQqefPJJLF68GEqlarDH6tWrMXHiREyePBn5+fn2i7jF+Uq5wbImhRKHzlYZLHfGknh7sfXQciVJ1sRrVnPw71xb/0/44Mw9Nc0m8U8//RQpKSloaFANvV2+fDnmz5+PL7/8EizL4sCBAygsLERubi62bduGd999F0uXLuU1SEvrr9YcPIepn+Ug/9J1Xt9fyOyVy+z5m7G9i6GDZ24dmkh5yj4C+ugORUjnjDFmk3hwcDBWrVql+buwsBCDBw8GAAwfPhyHDx9GXl4epFIpGIZBYGAgFAoFqqute5gxn67d0h/oYcnX9FPBZUz97Kh9AuoQwiuatHneeL1/t/05ne2hrd8SDRbSZ+kp1JmOm9kkHhcXB1dX7cBOlmU1B8DLyws1NTWora2FWCzWrKNezpf2uFg+tzEPh85etf8btTMhFTS01Skd/wOz/wRiLe/T8R+1U+gsU27YwuqGTRcX7SZyuRw+Pj4Qi8WQy+V6y729vfmJ0AatS3TmEtmFq9rYv/nDsefccDQsy+L5TXlIWHeEh32p/s/HiE21suv2e7Zh2/BbJ+7o12p7V1vUNyvsun9btUchyuokHhERgZycHABAVlYWJBIJYmJikJ2dDaVSibKyMiiVSvj5+fEWpLXHobFZqTe7Xutb62W7/kRI4k7N3w++lan598tbT9gSosOpqVdVKR3/2/L2gXvT9+GzX89btK5uaXnXycvIKeav+oyrdBr/wa9W7UP946msccxpVPn6casPlZDuuOwhbUchAKD8hv2eKt8W9rzjsjqJL1y4EKtWrUJCQgKampoQFxeHqKgoSCQSJCQkQCaTIS0tjdcgrT1Bn92Qhyc/zcHEjw5jy28XDbb/JMuyRGXMr39VapLk6cs1qLjpeCfOf1tmGbTGVXkjMnb+qbfsys16vLzlOOqbuEs6fOYOU/XYReU3rdre1rjaexBIW6uOrsobAWhnlbRFZU0DQhJ3Iue88KsTb9Y75sRn9jytLEriQUFB2Lp1KwAgNDQUGzduxJYtW7B8+XKIRCIAgEwmw7Zt27B9+3ZIJBJeg+zl09XqbXJLqvH7hWtYuP2kTT/owrIbOF9Zy/natPW5eHHzcQBA3PtZGLLsgA3vYF981Su/setPfHOsFD8V6E+Na2rvVbUNeHfvaSitnKZWfaJHpO2xMkr97YG2zydu73p5vibAqm5J4m2pBvy9RHUX9Z9DJSbXu1HXZPNxtTSJXbx6y6b9W/s+nYkgBvvwNzTZcmM/zMZD7/wCeUMz5+vnjCR4XfVNClzpoFI6Xw/cUc9rbqyUzPWjSdyejw9/PoujxdaV7Frv6reSapzgqA5qyw+14mY9rrUkvo6knTudr/21YVsL1mlsVmLg0r1I+a7A9jcyo1reiOFvHWzTPhx1sI9DVad0hOAenm3a3tpSim69cOTiPQhJ3InH1hzSW+fC1Vu4dE1bavgxv8xgPwNSf8LgZQcwcOle7DpZbvT9lErWpifklF2vw7A3f8ala7cwIHU33t17WudVy86aqtoGPPWfXFy/pU1sO46XIumbkwCA+iZVXLpJIv/SddQYubjpbtPW0vCkj49gwppDBvX6xh5E0XppNUeyHrLsAO5J38frk3+aFUrr7zpa/m9tr4r9RRUIX7TLoHChNJPF6xoVRtsHuHrKnL1Si5FvZ2r+bmw5P3ccL7UqXmvU8FAV0jL2EGszz0K68uc2708IBJHEh4b1aNP2macrrVq/db0wwN1AKF2pLTXM/fKY0f3dqGvC85v+wIajFxCSuBN7Wz21Jyx5F/ov2m0yprNXarE+u1hv2dbf/8bf1XXY+vsl1Dcp8eHPZ43WXRvzxMdHkHm6El/mXtQse3HzcXyl87cay7J4fO0hjF+tc0HjyB3qZGB16VBn/Z352ote6wvosl2G3w9XLK9uO4EJrbZV25RzgXN52XXDO6fGZiWSvjmJg6eucG7Tf9FuPLDiZ72LujmsNotb5e29p9GkYFFyVX/Esrkk/sS6I7jvjf24fKMekz85onfRVtPtyTPm/SwUV2nfw5Iw6xoVRu9yuKJTX2SLym6isdm2x/yFJO7ErUbtBU19HN786TQuXTPfM+lyq4bQrDOVRi9UTQql1W0m7XFnIIgkLhQhiTsxoeXJ4Lq9X9RSW25F324pMX+Zc9FkF7jlu/7E7pPleGzNIYx69xek/1ikKbGXVMm1pXudEyv5m5NgWRaHz2mnHwhJ3ImQxJ2cP9zzLT9Uc+cmywIVNxtw7KLlvV3Uu7wmb8QrW09ofmxVtQ3IPK1KiJ9knef8AaufbsTl4GltMj11+abRH9YNE0/3Kb1eh6/zLuHXvyo1x3TH8VLEvZ8FAKhtaMaS71U9HnadLMdXuRcx44vf8FcF9/iHyzfr9S7qFmNVdzZ/V5u/AJgqqZq7EThZegMA8PEv53D0fDW++UObqNSJ5sQl1TpKJWv8bkdncXGVHCdbtgGACWuycU/6PtOB6IhJ34dtv/+N+A9/xdIfCm3u611xU3uH0fpUOHr+qqZA8ltJNc60+v6W7z6l9/f0z7XtXbqu32pE+KLdkGTst7qgZG+UxHl24tIN/GOF6ds4eYPq1jb525N4QGfdkMSdeifIuqzzmLPpD4O7gPIbdRjxdibOVKjq5T/8+azO+1/Ht8dKcavR8ET77Ndig2WmvLRFezIbyxGlHBchdaNgzvmrqKptwHMb87D9j0v48IAqzv/7LAd/tFwMGpqV+PxQcct7WFZquXD1Ft7acwohiTsx5v1fseHoBc7tTZWa1v1yHq9uO4Fp63PRf9FujP3wV4ML1BeHS/BXRY3efi/frDfbziFvaMa+ogqD5Yu+Pam58Kr3mVtSjfGrD2HYm6YvAFW1Dbh7yV6cuqxKQg1WlFyNVdXtOF6K8EW7cKFVY+LazLMG62rurnSOxci3MzGupdACQHM+cnln72lU1RpW57y1R1Wg4brT1S1hm6Kb+lmwet/75E+OaqoGJ318BKPfyzLYPvW7AlypMf2dHj2vavy9Km/Ey1sNk7zaib+vIyRxp6axWH1hGr/6EH79y7oaAUtRErcDrsTW+vX73tjP+Zq5ftpKFrh/ufGLxLlKudG+7qsPnkVI4k5NItG9W+BKeN8e05bWPvv1PH7mqE6I/9Cw/3ZTS4JZm3kOkoz9mj7kP5+qwL6iCk0iUrtZ14TNuRfRpLD81nPNwXOaf6ftKMSuk+UG2//RKikXlt2AMYVl3CX62Pey9EqI09bnYvCyA5o6cHVpXddr2/PxzP9+x9krqqS2v6gCIYk7sSnnIp7f1HKHYcVddpNCib9aJcjnNuQZrLfjeCmaOBK27h2J+jO+/mMRXtx8HE0KVpNI1Q6fM2yQVh8DY9fF1sdux/FSvfNrbeY5JG4/abDdlZZ6+oqb9QbdA4vKzHcrBYD9f2ovmGcqarHip1MG63DdGattOHoByd+c1KtCLL+h/xvWbS/YddL4Q8x/OaNK1OoqXN2LXuvjzBdK4g7m7b1nTL5+lIe+vM9v+sPgpN5x3LBhVtepyzVI/tbwR6jrla0n0NisxBEjMTYpWDzzv98Nlq/NPIfEb0zv2xxNcjRh7IfZJl+vM3KbvNNIo3S1vNGgf/apyzdR0lJFpb6rat2+8GN+GWcOVyhZ/FRwWZMQy67XoUmhxLP/+x1TPtWf1+dKS99uXS9uPo43fzqFZoVSrxHTmosjwJ2o1UmsoVnJ2WC8MUf/M3JVSTSYGFVZVduIR1fpfz/yRgVGvfuLpi3i9OUaTXWlrj8uXtP7e90vpgtCW367iB9O6J/v+/+8gkU65/fsVhfJ/Rx3Vq2VXq/DqcuqC4/6eBWUai9E9uqg0qmedu8Mpn+ea5f9/nXFfJdJc7b/cQm+Hm5GX9dtKHNEW3/n7sXEVTVy4tJ1zhLrmPd/RaCvOwDVD3nM+1kGdx5zvzyGddPuNdi2X/IuAMD7CYMwcsBtelVtlvr012JcrL6FPYUVeGfSQNwd5KtXhWAunR89fxXNSsPS/NIftHccMen7ULJirObvW43NmvYeU6zte7859yLOXqnFjC9+Q+6ihzH1s6Ooqm1799CFHHcEgKors7phNP/SDVy6dgtB3T3Bsiy2cTy/oOJmPRqblejjp+o9p1uNuurns3hySLD+BnbqZ0hJnPBKXb/d2T2+9rDR18paejwwYAwSuFrrkp6u+VuO4/2EQTbHtqdQddF5ZZthtZq5RrnJn3DP5PlV7t9Gt2k9OOvp//7GuV7WmUqLp3UAgN06A8wGv2F8QJ2p6g1rtG7Mla48iKlDgtGXo4tz/qXrml5aJSvGctb3P2LlVBG2oiROiJ1wjR2w1PwtxhvP2sLY3Ya1TLUv7P+TuysmwN1915FtyjHsagtAr5utvKEZkgzDNq7rrafEttNwUqoTJ8RO1maeM7+SQJlrX3Am636x7Hs215ffVoJJ4nf26ripbQkhxBjdLr4dQTBJfPaDYR0dAiGE2Mxek3MJpk788Xtuh7yhGXf29kGIv6fJhg5CCHE0hRb2e7eWYEriDMNg2v0hGBzqh9u83ZH56oiODokQQjqcYJJ4ayH+XhjQm+rJCSHOTbBJHAB2vzgMJxaPxm3e1j80ghBCOhKbwP0AABRjSURBVANBJ3GGYeDr4YbJ9/WxafvcRQ/zHBEhhLQvQSdxtX63iTmXr3kyBpueHgIA+PLpIejmqT8k/DZvd6Q/FmX3+AghxF4E0zvFlPEDAxHmL8bdQb7YV1SBs1dqMequ2xDe0rdcPc/DTy8Ox5mKGkz/PBcvx94BAAb16q4ujN7w22OpsVbNkUwIIe3J5iT++OOPQyxWlYCDgoKQkJCAN954AyKRCFKpFHPnzuUtSHMYhsHdQb4AgNiIXoiN6MW5Xm9fd/T2ddebvOfu231xb9/uyLugmgnt7LJ4zexwIhcG3b26WBzHmMje+KmQn3kcCCHEEjYl8YaGBrAsiw0bNmiWTZgwAatWrUKfPn3w7LPPoqioCBEREbwFai/ubiJsn/MA53zD+19+0OL9fDpdghF39kR4y2PWNj87FENC/VDXpLD56e2EEGKOTXXip06dQl1dHWbOnInp06fjt99+Q2NjI4KDg8EwDKRSKQ4fNj7Lm6PbOvt+pD8WhVB/L73l6/8tAQBEBvoYbOPCAG4iF5xIG42Ns4ZgaFgPMAwDzy6uWPNkjGa9CYMCeYlRxNfj7AkhgmZTEnd3d8esWbOwfv16LF26FElJSfDw8NC87uXlhZoa7ik4HVWCpA8G9ukGABgc6odpQ/sarKOeDli3S6M6ofu0zKPt6+kGabi/3nZjowNwYvFoFC6NwweT77Epvnv7dtf7u3BpHOY91B/e7tqbqV3zhuGOXmJEBGgvMnf00jb6PiEJsum921tXVxf06+llfkVCiG1JPDQ0FOPHjwfDMAgNDYW3tzeuX9c+Cksul8PHx7C06shWTozGjhf+YfV2qY9GYO3UGNwX4mdyPV8PN3h11a+9Sn00AiUrxmr+e/ye29Hbxx1Jjwww2H77nAdwOmOM5m93NxFeHn0nTi6Jw6L4uwAAAb7u2PvSg/jm+Qc0690f1gMAcCTpIfz7gRCzn+ez6RKz67SmvvgBwIK4O63evrWCpXH4QSaFpNWFixBiyKYk/vXXX2PFihUAgIqKCtTV1cHT0xMXL14Ey7LIzs6GRGJ9MnBUx1JjkZv8MO4P88eQUD8ktyRNABgc4of4uwOs2t+aJ2Pg2UWE/xuq/+SP9xIG4Wjyw5j9YD/O7dxcuL+up4eF4tyyeE0jrLubSPNayqMR2P/ygwjw9UBkoC+KXo8z2P6Fkdr3G9WqUXhAb2/Me6i/0c9SsDRO7+L3/Ajtvt6ZNNCgXUH3ApP56gjsnCfFqfQxeuu4uqiqob6eo1339m4e2PzsUM4YdC8ihDgbm5L4xIkTUVNTgylTpuCll17CsmXLkJGRgVdffRUTJ05EREQEBg4cyHesHaa7Vxfc5uMOjy4ibJl9v6brIgC42FA3PTY6AEWvj0FXV5HRdZIeGYAfZVK9Zcae7sQwjNE6cjeRC/rr9KP37OKKf8bcrrfOgjj9kv+6afcivGWbUXf1wsujjZeuxS13F8PC/fHUAyFgGAZfPj0E9wR3w/hBgeh/mxg/zR+mWT8muDuKl8ejeHk8Qvy9EBnoq3fRUX8eNXUbgkcXEYa23FXoWvmvu/HVM0OMxkdIZ2dT75QuXbrgnXfeMVi+devWNgckFJ8/JcHt3Qwf28QX3dL4qLtUpWN1chsSarrqxhxXMxeeuMjeOHO5Bu/sO6P3tG5dEQE+CNepb98wS5tIH+jvj2/7a9sFBvTWr1rjetbimxOj8drX+QbL308YhDt6eWP8QFUyXzhmAA6fq8Kvf1UBABLuC0ZtQ7PJz+Moenp3xdi7Awweriw07m4u+Pf9IViXZfmj1oj9dIrBPh3hoQHcfdH5ptunHQD2vjQcgd08jKxtvawFIwGo5qHRLRG3zrP9enphkqQPPs06j3kPh1tUv26NJyR9OJM4wzB4YaS2OmfOiH6YM6IfDp+rQvhtqjsi9WOvxF1dNQnd1YXBkaSHseSHQuzM535avT0Nv6Mnss5Uav6ePTwMiY8MAMMwgk/i7z4xCFc5nnhvyrShfbHh6AU7ReTcOsWwe2dyRy9vTRWGrXQnpw9ueQjsXQE+Bl0qddc98MoIPPdgP+SlxvKewG3xQD9/9GzpJaQeYMsAeGZYKF4bcyfOLotHT++uenX0uozVr/MhZexd+N/MwXhtjLYaKqi7h+YO5PyyeM10ENbaOU9qfiUbZHBMPzHqrtsMlgV190D83QFofTP3+oRIzm1/lEkx4s6eesdC7Z1JhlWu0v7++OoZ+3037UG3d1h7oCTuxEw1CKoTDl8PIxkd0cvsPDVbnh2qV39uMXWQDLBobASeH6Etud/Vm/sH1bp+3dKBXa3bKbh4dlFdZJ8f0R9/vj4GC+LuxJTB2kZsFxcG/+jvb7Ddf2bcZ3DnBQBvTxqIN/8VjftCumNAbx+4u7lgxj9CeJ2K+f+G9sUD/fSPCVc7i5tIlTJaPy7xCYn+JHSvT4hCyYqxiLrdF1/MGAxvdzdNtaDav+4NwlOtCgT/mXEf7u9n2PbRuvHbkek23rcHSuKdVPbCkfjOTJfJqYODjb6mLk1EBfryEs8n0yWcfe91DQnrYVB/bgl1vb0LR127iwuDA6+YTtAlK8bCX2zZ9ApRt/siTKcPe0gP/XaR27y76vXH9+giwgsj+8NVZPqnNmVwMEbeaVjy/fb5BzDx3iA8cV8fbHvuAYhcGJxKfwSLx0Xip/nDNetNulf1nmmPWj5KeveL+hfM3j7uAFR13gDAQHU8/9G/B4qXx+NpaahmwJskxE8vATMMsHic9r25vovP/i3BG4/rX8hbx6u+SPz1xiOaC99jgwINGr87WtaCkXhySDC2tLqjO7lkNNzdRCheHm+wjb0G6FES76SCuntikJGStiWl65EDbkPWgpEYG21d98mOoP6BjxvIHauxrpm6rOlldD9HL5k3/xWNf8UEISf5YbMJW+39hEFm17kn2LK+8uoxCMZ6MHG5q9Vt/2P3qHotqS+2Iwf0BADE3x0AhmGQ8mgEwnpqG7OXjI9E9sKReHNiNLq6ivQ+t7E4+nTXv+i5uDDIfHUEHo0O0EuIbiIXLB4XgY+mxuD9lgFyYyJ7AwCeloZa/BmfM9Jdd8U/77Z4H1x6+7pj2eN3Y0hYD5xKH4NjqbEoXBoHb3fVoD+GYfTuvgBt2w3fqGHTmZn5wQf3sF/vGz65u4lwPC3WaFuBr4cb53Jd6pKjZxcRbjUqTK77+oQobMq5qLfsvlA/PGHlvPaP3XM75m85DgB6I1Tj7+6NXSetm0iNjwQx/I6eKFkxFizL4rF7bkdkoC8mDLodXV2NX5SCunviCYnqPFHqzP7Z3ZP7zkbZEucwnVHNIf5eWK0zNYWau5sIj+iMwVDfcVlaon3j8ShU1jToLRsY5IsTl26gm5H41JLjB2DZrlNGX++ic0zc3UScdwqpj96FXj5d8f7+vwAATw4xfufbFlQSJ51CN88uRkvAvq3mkR8Wblgfrc4LSpY1W5q1x21xr5aqDADw7qqKl6uh2RidZgFOk+4NwosPhwNQDeDaNc942wPDMIhsqUZzdxNxdgnlok7QTz0QopfkOOO05pZBs3/V/wN8VcfKx90VTz0Qgi9m3IffU0YZvhervThPGBSIjbOGIMDXo+U10xe9Z4dzl+AB1cAzS3h2ccX8UXdo7mbD/Lmfe9BWVBJ3QoEtPwJjpaXOKHvhSKw5eBYvjbpDM8+NLvWPXcnq995pL7o5TV3inD08zOLt1TEbS46DQ/0Q4u+FDw78Ba+urojgmMStrR68Q1X9oq6W4aTTk8ha6s8Y0M0Dv6eMgp9nF5PVYCy0F+eg7h6Qhvvjy1xVN0eFFV9yL5+uqLipKtHnLxltUfWcrp7irpp47IFK4k5I9nA4Vj95D2cXss4qqLsnlv8zGrf5uHPe+qpzH8uyCOrOXz98SzE6aU2bkC3f/s6Wnip9e3jiR5kU81pK3WoujPYddEuh/+hvWL9vq7CeYpSsGGu0LQYA7gnuBnc3F72+/5aaPyocQd09MDSsB/zFXc23Y7Cs5qKmUKoW6V6szVFPHtdPpx3Ax90NHl2sa2SdPFhVzTbayHMO2oqSuBNyE7ng0ehAm25pOyt16Wra0BB8/Vz7dRF7e9JADA71w8M6F1Rt1Yjl38/UIcH4Ya4UI+68DVG3+2qeXKUmcmG0Fyqd5Z8/dR/yOKoi7KWbZxecSn8Eg20YdRx1uy+yFz5ktI1j2eOGjZXqpK2+cGmSuAVZfPG4yJZtrQ5Vz4DePihZMRZ9/OzTxkRJnBCoekmcfeMRpD56F3r7umPfS8PNbwRgwiBV1YGfjVVTE+8NwtbZ9+vdHainF3a3osSn+3QrLmE9vTR1sv++P0SzvKurCD3EXY1sJSz3hej35IkO6qapTlHnYfXU0ep6dT+OJ3epX1NPOR0d5IsPJg/iHJzkCKhOnJAWug2j4b280beHJy5cvaVZdibjEc2/DyU+hK6uLujh1QWzHwzTDPDhw4K4O9HLxx1jrZwd05ToIFUVB9dgos5Cd2K6otfj4NnFVXP3oa6vf2ZYGO7v1wPRQd1QuDQOIhcGA1J/0tuPenxFeC9v/CiTYkBvb4u7jXYESuKEGNH6Nlq3x4VuDwU+E7h6f8b6NxPT/MVdUFPfrPlOooO64fyyeE39uYsLo7mgtZ7fX023p1DU7fwMdrMnSuLEaQ0N80NsRO+ODqPNjPWP3zhrCD7+5ZzZ6Q46k6NJDxv0ArFlumghoSROnNbmZ+83+bqxaXgdyb6XhmsmAmtNGu5v8KjAzs6Wao9Z0lCwLPD5oWLO5+c6OkrihJix+dmhaFL3UXMw4b34mwTLWaW2zN/y2pg7BfkAcsetrSekg00YqOp5cleAD4aF9+zgaIi9ubuJNBNwCQmVxAkx4uXYO/DciH5tnr+dEHsS3mWHkHbi4sJQAicOj5I4IYQIGK/FDKVSiSVLluD06dPo0qULMjIy0Lev6QcBEEIIsR2vJfH9+/ejsbERW7ZswSuvvIIVK1bwuXtCCCGt8JrE8/LyMGyYap7iQYMGoaCggM/dE0IIaYXXJF5bWwuxWDtto0gkQnNzM59vQQghRAevSVwsFkMul2v+ViqVcHWl1n1CCLEXXjNsTEwMDh48iPj4eBw/fhx33KGd01ihUD238PJl654dSAghzkydM9U5tDVek3hsbCwOHTqEyZMng2VZLFu2TPNaZWUlAGDq1Kl8viUhhDiFyspKzt5+DMvHY7ItUF9fj4KCAvTs2RMikXWPNyKEEGelUChQWVmJqKgouLu7G7zebkmcEEII/2jEJiGECJjDdx1xhFGgjz/+uKbrZFBQEBISEvDGG29AJBJBKpVi7ty5RuM8fvy4wbp8OXHiBN5++21s2LABFy5cQGJiIhiGQXh4OBYvXgwXFxesXr0amZmZcHV1RXJyMqKjo61al88Yi4qKMHv2bISEhAAApkyZgvj4+A6JsampCcnJySgtLUVjYyPmzJmD/v37O8wx5IovICDAYY4foLrNT0lJQXFxMRiGwdKlS9G1a1eHOYZc8TU3NzvUMeQF6+D27NnDLly4kGVZlj127Bj73HPPtev719fXsxMmTNBbNn78ePbChQusUqlkn376abawsNBonFzr8uGTTz5hH330UXbSpEksy7Ls7Nmz2aNHj7Isy7Kpqans3r172YKCAnbatGmsUqlkS0tL2X/+859Wr8tnjFu3bmXXr1+vt05Hxfj111+zGRkZLMuy7LVr19gHH3zQoY4hV3yOdPxYlmX37dvHJiYmsizLskePHmWfe+45hzqGXPE52jHkg8OXxDt6FOipU6dQV1eHmTNnorm5GTKZDI2NjQgODgYASKVSHD58GJWVlQZx1tbWcq4bERHR5riCg4OxatUqvPbaawCAwsJCDB48GAAwfPhwHDp0CKGhoZBKpWAYBoGBgVAoFKiurrZqXT8/P95iLCgoQHFxMQ4cOIC+ffsiOTkZeXl5HRLjmDFjEBcXBwBgWRYikcihjiFXfI50/ABg1KhRGDFiBACgrKwMPj4+OHz4sMMcQ674HO0Y8sHh68Q7ehSou7s7Zs2ahfXr12Pp0qVISkqCh4f2IbleXl6oqanhjLP1MvW6fIiLi9MbSMWyLJiWR3sbi0m93Jp1+YwxOjoar732GjZt2oQ+ffpgzZo1HRajl5cXxGIxamtrMW/ePMyfP9+hjiFXfI50/NRcXV2xcOFCpKenY9y4cQ51DLnic8Rj2FYOn8Q7ehRoaGgoxo8fD4ZhEBoaCm9vb1y/fl3zulwuh4+PD2ecrZep17UHFxftV2ksJrlcDm9vb6vW5VNsbCyioqI0/y4qKurQGMvLyzF9+nRMmDAB48aNc7hj2Do+Rzt+aitXrsSePXuQmpqKhoYGi9+3vWLUjU8qlTrkMWwLh0/iMTExyMrKAgCDUaDt4euvv9bMxlhRUYG6ujp4enri4sWLYFkW2dnZkEgknHGKxWK4ubkZrGsPERERyMnJAQBkZWVpYsrOzoZSqURZWRmUSiX8/PysWpdPs2bNQn5+PgDgyJEjiIyM7LAYq6qqMHPmTCxYsAATJ04E4FjHkCs+Rzp+APDdd99h3bp1AAAPDw8wDIOoqCiHOYZc8c2dO9ehjiEfHL6fuLrXx5kzZzSjQPv169du79/Y2IikpCSUlZWBYRi8+uqrcHFxwbJly6BQKCCVSvHSSy8ZjfP48eMG6/Ll0qVLePnll7F161YUFxcjNTUVTU1NCAsLQ0ZGBkQiEVatWoWsrCwolUokJSVBIpFYtS6fMRYWFiI9PR1ubm7w9/dHeno6xGJxh8SYkZGB3bt3IywsTLNs0aJFyMjIcIhjyBXf/Pnz8dZbbznE8QOAW7duISkpCVVVVWhubsYzzzyDfv36Ocx5yBVfQECAw5yDfHH4JE4IIcQ4h69OIYQQYhwlcUIIETBK4oQQImCUxAkhRMAoiRNCiIBREieEEAGjJE4IIQJGSZwQQgTs/wG0xSrxW56HzwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " erd waul ced? and of the cewoth oN had to thed a the veal thasg saseatsered, sord washe cof offill Do pyon'e flandyouse ace whal reny: heed s, hintefwiny couge by sapan te wale to to toen her ion, mm \n",
      "----\n",
      "iter 37806, loss 53.939671\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        with DelayedKeyboardInterrupt():\n",
    "            # Reset\n",
    "            if pointer + T_steps >= len(data) or iteration == 0:\n",
    "                g_h_prev = np.zeros((H_size, 1))\n",
    "                g_C_prev = np.zeros((H_size, 1))\n",
    "                pointer = 0\n",
    "\n",
    "\n",
    "            inputs = ([char_to_idx[ch] \n",
    "                       for ch in data[pointer: pointer + T_steps]])\n",
    "            targets = ([char_to_idx[ch] \n",
    "                        for ch in data[pointer + 1: pointer + T_steps + 1]])\n",
    "\n",
    "            loss, g_h_prev, g_C_prev = \\\n",
    "                forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "            # Print every hundred steps\n",
    "            if iteration % 100 == 0:\n",
    "                update_status(inputs, g_h_prev, g_C_prev)\n",
    "\n",
    "            update_paramters()\n",
    "\n",
    "            plot_iter = np.append(plot_iter, [iteration])\n",
    "            plot_loss = np.append(plot_loss, [loss])\n",
    "\n",
    "            pointer += T_steps\n",
    "            iteration += 1\n",
    "    except KeyboardInterrupt:\n",
    "        update_status(inputs, g_h_prev, g_C_prev)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Check\n",
    "\n",
    "Approximate the numerical gradients by changing parameters and running the model. Check if the approximated gradients are equal to the computed analytical gradients (by backpropagation).\n",
    "\n",
    "Try this on `num_checks` individual paramters picked randomly for each weight matrix and bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate numerical gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_numerical_gradient(param, idx, delta, inputs, target, h_prev, C_prev):\n",
    "    old_val = param.v.flat[idx]\n",
    "    \n",
    "    # evaluate loss at [x + delta] and [x - delta]\n",
    "    param.v.flat[idx] = old_val + delta\n",
    "    loss_plus_delta, _, _ = forward_backward(inputs, targets,\n",
    "                                             h_prev, C_prev)\n",
    "    param.v.flat[idx] = old_val - delta\n",
    "    loss_mins_delta, _, _ = forward_backward(inputs, targets, \n",
    "                                             h_prev, C_prev)\n",
    "    \n",
    "    param.v.flat[idx] = old_val #reset\n",
    "\n",
    "    grad_numerical = (loss_plus_delta - loss_mins_delta) / (2 * delta)\n",
    "    # Clip numerical error because analytical gradient is clipped\n",
    "    [grad_numerical] = np.clip([grad_numerical], -1, 1) \n",
    "    \n",
    "    return grad_numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient of each paramter matrix/vector at `num_checks` individual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(num_checks, delta, inputs, target, h_prev, C_prev):\n",
    "    global parameters\n",
    "    \n",
    "    # To calculate computed gradients\n",
    "    _, _, _ =  forward_backward(inputs, targets, h_prev, C_prev)\n",
    "    \n",
    "    \n",
    "    for param in parameters.all():\n",
    "        #Make a copy because this will get modified\n",
    "        d_copy = np.copy(param.d)\n",
    "\n",
    "        # Test num_checks times\n",
    "        for i in range(num_checks):\n",
    "            # Pick a random index\n",
    "            rnd_idx = int(uniform(0, param.v.size))\n",
    "            \n",
    "            grad_numerical = calc_numerical_gradient(param,\n",
    "                                                     rnd_idx,\n",
    "                                                     delta,\n",
    "                                                     inputs,\n",
    "                                                     target,\n",
    "                                                     h_prev, C_prev)\n",
    "            grad_analytical = d_copy.flat[rnd_idx]\n",
    "\n",
    "            err_sum = abs(grad_numerical + grad_analytical) + 1e-09\n",
    "            rel_error = abs(grad_analytical - grad_numerical) / err_sum\n",
    "            \n",
    "            # If relative error is greater than 1e-06\n",
    "            if rel_error > 1e-06:\n",
    "                print('%s (%e, %e) => %e'\n",
    "                      % (param.name, grad_numerical, grad_analytical, rel_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_o (4.547474e-08, 4.448348e-08) => 1.089789e-02\n",
      "W_o (0.000000e+00, 7.775965e-11) => 7.214935e-02\n",
      "W_o (-3.979039e-08, -3.898314e-08) => 1.011936e-02\n",
      "W_o (-1.136868e-08, -1.161067e-08) => 1.009125e-02\n",
      "W_o (-1.136868e-08, -1.118194e-08) => 7.929614e-03\n",
      "W_o (4.121148e-07, 4.117866e-07) => 3.978795e-04\n",
      "b_o (1.421085e-09, 2.143387e-09) => 1.582442e-01\n",
      "b_o (-8.739676e-08, -8.761852e-08) => 1.259919e-03\n",
      "b_o (3.268497e-08, 3.266015e-08) => 3.741031e-04\n",
      "b_o (-7.105427e-10, -1.090692e-10) => 3.305504e-01\n",
      "b_o (6.949108e-07, 6.953177e-07) => 2.924733e-04\n",
      "b_o (-2.131628e-09, -2.739074e-09) => 1.034707e-01\n",
      "b_o (1.136868e-08, 1.161069e-08) => 1.009216e-02\n",
      "b_o (2.842171e-09, 3.376894e-09) => 7.407099e-02\n",
      "b_o (-1.136868e-08, -1.134988e-08) => 7.928564e-04\n"
     ]
    }
   ],
   "source": [
    "gradient_check(10, 1e-5, inputs, targets, g_h_prev, g_C_prev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
