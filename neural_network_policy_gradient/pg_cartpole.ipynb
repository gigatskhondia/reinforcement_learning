{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Specify the neural network architecture\n",
    "n_inputs = 4 # == env.observation_space.shape[0]\n",
    "n_hidden = 4 # it's a simple task, we don't need more hidden neurons \n",
    "n_outputs = 1 # only outputs the probability of accelerating left \n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-21b0e3342f2a>:9: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n"
     ]
    }
   ],
   "source": [
    "# 2. Build the neural network\n",
    "X_ = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X_\")\n",
    "hidden = fully_connected(X_, n_hidden, activation_fn=tf.nn.elu, weights_initializer=initializer) \n",
    "logits = fully_connected(hidden, n_outputs, activation_fn=None, weights_initializer=initializer) \n",
    "outputs = tf.nn.sigmoid(logits, name=\"Y_proba\")\n",
    "\n",
    "# 3. Select a random action based on the estimated probabilities\n",
    "p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs]) \n",
    "action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-8f8dde413370>:1: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "y = 1. - tf.to_float(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits( labels=y, logits=logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate) \n",
    "grads_and_vars = optimizer.compute_gradients(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = [grad for grad, variable in grads_and_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_placeholders = [] \n",
    "grads_and_vars_feed = []\n",
    "for grad, variable in grads_and_vars:\n",
    "    gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape()) \n",
    "    gradient_placeholders.append(gradient_placeholder) \n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "training_op = optimizer.apply_gradients(grads_and_vars_feed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate=0.95): \n",
    "    discounted_rewards = np.empty(len(rewards)) \n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards \n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate=0.95): \n",
    "    all_discounted_rewards = [discount_rewards(rewards) for rewards in all_rewards] \n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 251 # number of training iterations\n",
    "n_max_steps = 1000 # max steps per episode\n",
    "n_games_per_update = 10 # train the policy every 10 episodes \n",
    "save_iterations = 10 # save the model every 10 training iterations discount_rate = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch \n",
      "\n",
      "10 epoch \n",
      "\n",
      "20 epoch \n",
      "\n",
      "30 epoch \n",
      "\n",
      "40 epoch \n",
      "\n",
      "50 epoch \n",
      "\n",
      "60 epoch \n",
      "\n",
      "70 epoch \n",
      "\n",
      "80 epoch \n",
      "\n",
      "90 epoch \n",
      "\n",
      "100 epoch \n",
      "\n",
      "110 epoch \n",
      "\n",
      "120 epoch \n",
      "\n",
      "130 epoch \n",
      "\n",
      "140 epoch \n",
      "\n",
      "150 epoch \n",
      "\n",
      "160 epoch \n",
      "\n",
      "170 epoch \n",
      "\n",
      "180 epoch \n",
      "\n",
      "190 epoch \n",
      "\n",
      "200 epoch \n",
      "\n",
      "210 epoch \n",
      "\n",
      "220 epoch \n",
      "\n",
      "230 epoch \n",
      "\n",
      "240 epoch \n",
      "\n",
      "250 epoch \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: \n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        all_rewards = [] # all sequences of raw rewards for each episode \n",
    "        all_gradients = [] # gradients saved at each step of each episode \n",
    "        for game in range(n_games_per_update):\n",
    "            current_rewards = [] # all raw rewards from the current episode \n",
    "            current_gradients = [] # all gradients from the current episode\n",
    "            obs = env.reset()\n",
    "            for step in range(n_max_steps):\n",
    "                action_val, gradients_val = sess.run( [action, gradients], \n",
    "                feed_dict={X_: obs.reshape(1,n_inputs)}) # one obs \n",
    "                obs, reward, done, info = env.step(action_val[0][0]) \n",
    "                current_rewards.append(reward) \n",
    "                current_gradients.append(gradients_val)\n",
    "                if done:\n",
    "                    break\n",
    "            all_rewards.append(current_rewards) \n",
    "            all_gradients.append(current_gradients)\n",
    "            \n",
    "        # At this point we have run the policy for 10 episodes, and we are # ready for a policy update using the algorithm described earlier. \n",
    "        all_rewards = discount_and_normalize_rewards(all_rewards) \n",
    "        feed_dict = {}\n",
    "        for var_index, grad_placeholder in enumerate(gradient_placeholders):\n",
    "            # multiply the gradients by the action scores, and compute the mean \n",
    "            mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index] \n",
    "            for game_index, rewards in enumerate(all_rewards) \n",
    "            for step, reward in enumerate(rewards)],axis=0)\n",
    "            feed_dict[grad_placeholder] = mean_gradients\n",
    "        sess.run(training_op, feed_dict=feed_dict) \n",
    "        if iteration % save_iterations == 0:\n",
    "            print('{} epoch \\n'.format(iteration))\n",
    "            saver.save(sess, \"./ckpt/my_policy_net_pg.ckpt\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.import_meta_graph('./ckpt/my_policy_net_pg.ckpt.meta')\n",
    "        saver.restore(sess, \"./ckpt/my_policy_net_pg.ckpt\") \n",
    "\n",
    "        graph = tf.get_default_graph()\n",
    "        outputs = graph.get_tensor_by_name(\"Y_proba:0\") \n",
    "        X_ = graph.get_tensor_by_name(\"X_:0\") \n",
    "        \n",
    "        ans=[]\n",
    "        epoch=20\n",
    "        steps=1000\n",
    "        for i_episode in range(epoch):\n",
    "            obs = env.reset()\n",
    "            for t in range(steps):\n",
    "                env.render()\n",
    "                output=sess.run([outputs],feed_dict={X_: obs.reshape(1,n_inputs)})\n",
    "                action = np.log([output[0][0][0], 1 - output[0][0][0]])\n",
    "                obs, reward, done, info = env.step(np.argmax(action))\n",
    "                if done or t==steps-1:\n",
    "                    print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "                    ans.append(t+1)\n",
    "                    break\n",
    "        env.close()\n",
    "    return sum(ans)/epoch            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ckpt/my_policy_net_pg.ckpt\n",
      "Episode finished after 387 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 267 timesteps\n",
      "Episode finished after 313 timesteps\n",
      "Episode finished after 280 timesteps\n",
      "Episode finished after 307 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 270 timesteps\n",
      "Episode finished after 291 timesteps\n",
      "Episode finished after 306 timesteps\n",
      "Episode finished after 334 timesteps\n",
      "Episode finished after 340 timesteps\n",
      "Episode finished after 385 timesteps\n",
      "Episode finished after 358 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 288 timesteps\n",
      "Episode finished after 264 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 380 timesteps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "363.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
