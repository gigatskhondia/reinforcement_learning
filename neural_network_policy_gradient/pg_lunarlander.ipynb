{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Specify the neural network architecture\n",
    "n_inputs = 8 # == env.observation_space.shape[0]\n",
    "n_hidden = 10 # it's a simple task, we don't need more hidden neurons \n",
    "n_outputs = 4 # only outputs the probability of accelerating left \n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-0ae83d47ec75>:8: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n"
     ]
    }
   ],
   "source": [
    "# 2. Build the neural network\n",
    "X_ = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X_\")\n",
    "hidden = fully_connected(X_, n_hidden, activation_fn=tf.nn.elu, weights_initializer=initializer) \n",
    "logits = fully_connected(hidden, n_outputs, activation_fn=None, weights_initializer=initializer) \n",
    "outputs = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "# 3. Select a random action based on the estimated probabilities\n",
    "action = tf.multinomial(tf.log(outputs), num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=tf.reshape(tf.one_hot(action,depth=4,dtype=tf.float32),[4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=tf.transpose(logits))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate) \n",
    "grads_and_vars = optimizer.compute_gradients(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = [grad for grad, variable in grads_and_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_placeholders = [] \n",
    "grads_and_vars_feed = []\n",
    "for grad, variable in grads_and_vars:\n",
    "    gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape()) \n",
    "    gradient_placeholders.append(gradient_placeholder) \n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "training_op = optimizer.apply_gradients(grads_and_vars_feed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate=0.95): \n",
    "    discounted_rewards = np.empty(len(rewards)) \n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards \n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate=0.95): \n",
    "    all_discounted_rewards = [discount_rewards(rewards) for rewards in all_rewards] \n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1001 # number of training iterations\n",
    "n_max_steps = 1000 # max steps per episode\n",
    "n_games_per_update = 10 # train the policy every 10 episodes \n",
    "save_iterations = 10 # save the model every 10 training iterations discount_rate = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch \n",
      "\n",
      "10 epoch \n",
      "\n",
      "20 epoch \n",
      "\n",
      "30 epoch \n",
      "\n",
      "40 epoch \n",
      "\n",
      "50 epoch \n",
      "\n",
      "60 epoch \n",
      "\n",
      "70 epoch \n",
      "\n",
      "80 epoch \n",
      "\n",
      "90 epoch \n",
      "\n",
      "100 epoch \n",
      "\n",
      "110 epoch \n",
      "\n",
      "120 epoch \n",
      "\n",
      "130 epoch \n",
      "\n",
      "140 epoch \n",
      "\n",
      "150 epoch \n",
      "\n",
      "160 epoch \n",
      "\n",
      "170 epoch \n",
      "\n",
      "180 epoch \n",
      "\n",
      "190 epoch \n",
      "\n",
      "200 epoch \n",
      "\n",
      "210 epoch \n",
      "\n",
      "220 epoch \n",
      "\n",
      "230 epoch \n",
      "\n",
      "240 epoch \n",
      "\n",
      "250 epoch \n",
      "\n",
      "260 epoch \n",
      "\n",
      "270 epoch \n",
      "\n",
      "280 epoch \n",
      "\n",
      "290 epoch \n",
      "\n",
      "300 epoch \n",
      "\n",
      "310 epoch \n",
      "\n",
      "320 epoch \n",
      "\n",
      "330 epoch \n",
      "\n",
      "340 epoch \n",
      "\n",
      "350 epoch \n",
      "\n",
      "360 epoch \n",
      "\n",
      "370 epoch \n",
      "\n",
      "380 epoch \n",
      "\n",
      "390 epoch \n",
      "\n",
      "400 epoch \n",
      "\n",
      "410 epoch \n",
      "\n",
      "420 epoch \n",
      "\n",
      "430 epoch \n",
      "\n",
      "440 epoch \n",
      "\n",
      "450 epoch \n",
      "\n",
      "460 epoch \n",
      "\n",
      "470 epoch \n",
      "\n",
      "480 epoch \n",
      "\n",
      "490 epoch \n",
      "\n",
      "500 epoch \n",
      "\n",
      "510 epoch \n",
      "\n",
      "520 epoch \n",
      "\n",
      "530 epoch \n",
      "\n",
      "540 epoch \n",
      "\n",
      "550 epoch \n",
      "\n",
      "560 epoch \n",
      "\n",
      "570 epoch \n",
      "\n",
      "580 epoch \n",
      "\n",
      "590 epoch \n",
      "\n",
      "600 epoch \n",
      "\n",
      "610 epoch \n",
      "\n",
      "620 epoch \n",
      "\n",
      "630 epoch \n",
      "\n",
      "640 epoch \n",
      "\n",
      "650 epoch \n",
      "\n",
      "660 epoch \n",
      "\n",
      "670 epoch \n",
      "\n",
      "680 epoch \n",
      "\n",
      "690 epoch \n",
      "\n",
      "700 epoch \n",
      "\n",
      "710 epoch \n",
      "\n",
      "720 epoch \n",
      "\n",
      "730 epoch \n",
      "\n",
      "740 epoch \n",
      "\n",
      "750 epoch \n",
      "\n",
      "760 epoch \n",
      "\n",
      "770 epoch \n",
      "\n",
      "780 epoch \n",
      "\n",
      "790 epoch \n",
      "\n",
      "800 epoch \n",
      "\n",
      "810 epoch \n",
      "\n",
      "820 epoch \n",
      "\n",
      "830 epoch \n",
      "\n",
      "840 epoch \n",
      "\n",
      "850 epoch \n",
      "\n",
      "860 epoch \n",
      "\n",
      "870 epoch \n",
      "\n",
      "880 epoch \n",
      "\n",
      "890 epoch \n",
      "\n",
      "900 epoch \n",
      "\n",
      "910 epoch \n",
      "\n",
      "920 epoch \n",
      "\n",
      "930 epoch \n",
      "\n",
      "940 epoch \n",
      "\n",
      "950 epoch \n",
      "\n",
      "960 epoch \n",
      "\n",
      "970 epoch \n",
      "\n",
      "980 epoch \n",
      "\n",
      "990 epoch \n",
      "\n",
      "1000 epoch \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: \n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        all_rewards = [] # all sequences of raw rewards for each episode \n",
    "        all_gradients = [] # gradients saved at each step of each episode \n",
    "        for game in range(n_games_per_update):\n",
    "            current_rewards = [] # all raw rewards from the current episode \n",
    "            current_gradients = [] # all gradients from the current episode\n",
    "            obs = env.reset()\n",
    "            for step in range(n_max_steps):\n",
    "                action_val, gradients_val = sess.run( [action, gradients], \n",
    "                feed_dict={X_: obs.reshape(1,n_inputs)}) # one obs \n",
    "                obs, reward, done, info = env.step(action_val[0][0]) \n",
    "                current_rewards.append(reward) \n",
    "                current_gradients.append(gradients_val)\n",
    "                if done:\n",
    "                    break\n",
    "            all_rewards.append(current_rewards) \n",
    "            all_gradients.append(current_gradients)\n",
    "            \n",
    "        # At this point we have run the policy for 10 episodes, and we are # ready for a policy update using the algorithm described earlier. \n",
    "        all_rewards = discount_and_normalize_rewards(all_rewards) \n",
    "        feed_dict = {}\n",
    "        for var_index, grad_placeholder in enumerate(gradient_placeholders):\n",
    "            # multiply the gradients by the action scores, and compute the mean \n",
    "            mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index] \n",
    "            for game_index, rewards in enumerate(all_rewards) \n",
    "            for step, reward in enumerate(rewards)],axis=0)\n",
    "            feed_dict[grad_placeholder] = mean_gradients\n",
    "        sess.run(training_op, feed_dict=feed_dict) \n",
    "        if iteration % save_iterations == 0:\n",
    "            print('{} epoch \\n'.format(iteration))\n",
    "            saver.save(sess, \"./ckpt_1/my_policy_1_net_pg.ckpt\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.import_meta_graph('./ckpt_1/my_policy_1_net_pg.ckpt.meta')\n",
    "        saver.restore(sess, \"./ckpt_1/my_policy_1_net_pg.ckpt\") \n",
    "\n",
    "        graph = tf.get_default_graph()\n",
    "        outputs = graph.get_tensor_by_name(\"Y_proba:0\") \n",
    "        X_ = graph.get_tensor_by_name(\"X_:0\") \n",
    "        \n",
    "        ans=[]\n",
    "        epoch=20\n",
    "        steps=1000\n",
    "        for i_episode in range(epoch):\n",
    "            obs = env.reset()\n",
    "            for t in range(steps):\n",
    "                env.render()\n",
    "                output=sess.run([outputs],feed_dict={X_: obs.reshape(1,n_inputs)})\n",
    "                action = np.log(output)\n",
    "                obs, reward, done, info = env.step(np.argmax(action))\n",
    "                if done or t==steps-1:\n",
    "                    print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "                    ans.append(t+1)\n",
    "                    break\n",
    "        env.close()\n",
    "    return sum(ans)/epoch            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ckpt_1/my_policy_1_net_pg.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 98 timesteps\n",
      "Episode finished after 71 timesteps\n",
      "Episode finished after 65 timesteps\n",
      "Episode finished after 78 timesteps\n",
      "Episode finished after 56 timesteps\n",
      "Episode finished after 82 timesteps\n",
      "Episode finished after 62 timesteps\n",
      "Episode finished after 90 timesteps\n",
      "Episode finished after 69 timesteps\n",
      "Episode finished after 55 timesteps\n",
      "Episode finished after 85 timesteps\n",
      "Episode finished after 64 timesteps\n",
      "Episode finished after 72 timesteps\n",
      "Episode finished after 88 timesteps\n",
      "Episode finished after 69 timesteps\n",
      "Episode finished after 61 timesteps\n",
      "Episode finished after 72 timesteps\n",
      "Episode finished after 79 timesteps\n",
      "Episode finished after 110 timesteps\n",
      "Episode finished after 54 timesteps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
