{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridWorldEnvironment import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating gridworld environment\n",
    "gw = GridWorld(gamma = .9, theta = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_action_value(env):\n",
    "    q = dict()\n",
    "    for state, action, next_state, reward in env.transitions:\n",
    "        q[(state, action)] = np.random.normal()\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(env, e, q, state):\n",
    "    actions = env.actions\n",
    "    action_values = []\n",
    "    prob = []\n",
    "    for action in actions:\n",
    "        action_values.append(q[(state, action)])\n",
    "    for i in range(len(action_values)):\n",
    "        if i == np.argmax(action_values):\n",
    "            prob.append(1 - e + e/len(action_values))\n",
    "        else:\n",
    "            prob.append(e/len(action_values))\n",
    "    return actions, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_e_greedy_policy(env, e, Q):\n",
    "    pi = dict()\n",
    "    for state in env.states:\n",
    "        pi[state] = e_greedy(env, e, Q, state)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_policy(env):\n",
    "    pi = dict()\n",
    "    for state in env.states:\n",
    "        actions = []\n",
    "        prob = []\n",
    "        for action in env.actions:\n",
    "            actions.append(action)\n",
    "            prob.append(0.25)\n",
    "        pi[state] = (actions, prob)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_over_actions(pi, Q, state):\n",
    "    actions, probs = pi[state]\n",
    "    q_values = np.zeros(4)\n",
    "    for s, a in Q.keys():\n",
    "        if s == state:\n",
    "            q_values[actions.index(a)] = Q[s,a]\n",
    "    return np.dot(q_values, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Unifying Algorithm:  Off-policy n-step Q(σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_Qσ(env, b, epsilon, alpha, n, num_iter, learn_pi = True):\n",
    "    Q = state_action_value(env)\n",
    "    Q_, pi_, delta, sigma, b_ , rho_ = dict(), dict(), dict(), dict(), dict(), dict()\n",
    "    pi = generate_e_greedy_policy(env, epsilon, Q)\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        current_state = np.random.choice(env.states)\n",
    "        action = np.random.choice(b[current_state][0], p = b[current_state][1])\n",
    "        state_trace, action_trace, reward_trace  = [current_state], [action], [0]\n",
    "        Q_[0] = Q[current_state, action]\n",
    "        sigma[0]=np.random.random(1)[0]\n",
    "        rho_[0]=1\n",
    "        \n",
    "        t, T = 0, 10000\n",
    "        while True:\n",
    "            if t < T:\n",
    "                next_state, reward = env.state_transition(current_state, action)\n",
    "                state_trace.append(next_state)\n",
    "                reward_trace.append(reward)\n",
    "                if next_state == 0:\n",
    "                    T = t + 1\n",
    "                    delta[t] = reward - Q_[t]\n",
    "                else:\n",
    "                    action = np.random.choice(b[next_state][0], p = b[next_state][1])\n",
    "                    action_trace.append(action)\n",
    "                    sigma[t+1]=np.random.random(1)[0]\n",
    "                    Q_[t+1] = Q[next_state, action]\n",
    "                    \n",
    "                    delta[t] = reward +env.gamma*sigma[t+1]*Q_[t+1] \\\n",
    "                    + env.gamma *(1-sigma[t+1])* avg_over_actions(pi, Q, next_state) - Q_[t]\n",
    "                    \n",
    "                    pi_[t+1] = pi[next_state][1][pi[next_state][0].index(action)]\n",
    "                    b_[t+1]= b[next_state][1][b[next_state][0].index(action)]\n",
    "                    rho_[t+1]=pi_[t+1]/b_[t+1]\n",
    "                   \n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                rho = 1\n",
    "                Z = 1\n",
    "                G = Q_[tau]\n",
    "                for i in range(tau, min([tau + n -1, T-1])):\n",
    "                    G+=Z*delta[i]\n",
    "                    Z=env.gamma*Z*((1-sigma[i+1])*pi_[i+1]+sigma[i+1])\n",
    "                    rho*=(1-sigma[i]+sigma[i+1]*rho_[i])\n",
    "                Q[state_trace[tau], action_trace[tau]] += alpha *rho* (G - Q[state_trace[tau], action_trace[tau]])\n",
    "                if learn_pi:\n",
    "                    pi[state_trace[tau]] = e_greedy(env, epsilon, Q, state_trace[tau])\n",
    "                    \n",
    "            current_state = next_state  \n",
    "            \n",
    "            if tau == (T-1):\n",
    "                break\n",
    "            t += 1\n",
    "            \n",
    "    return pi, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any policy with random probabilities > 0\n",
    "b = generate_random_policy(gw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, Q = n_step_Qσ(gw, b, 0.2, 0.1, 2, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_policy(pi, env):\n",
    "    temp = np.zeros(len(env.states) + 2)\n",
    "    for s in env.states:\n",
    "        a = pi[s][0][np.argmax(pi[s][1])]\n",
    "        if a == \"U\":\n",
    "            temp[s] = 0.25\n",
    "        elif a == \"D\":\n",
    "            temp[s] = 0.5\n",
    "        elif a == \"R\":\n",
    "            temp[s] = 0.75\n",
    "        else:\n",
    "            temp[s] = 1.0\n",
    "            \n",
    "    temp = temp.reshape(4,4)\n",
    "    ax = seaborn.heatmap(temp, cmap = \"prism\", linecolor=\"#282828\", cbar = False, linewidths = 0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### RED = TERMINAL (0)\n",
    "### GREEN = LEFT\n",
    "### BLUE = UP\n",
    "### PURPLE = RIGHT\n",
    "### ORANGE = DOWN\n",
    "\n",
    "show_policy(pi, gw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
