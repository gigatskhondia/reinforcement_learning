{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridWorldEnvironment import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating gridworld environment\n",
    "gw = GridWorld(gamma = .9, theta = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_action_value(env):\n",
    "    q = dict()\n",
    "    for state, action, next_state, reward in env.transitions:\n",
    "        q[(state, action)] = np.random.normal()\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_greedy_policy(env, Q):\n",
    "    pi = dict()\n",
    "    for state in env.states:\n",
    "        actions = []\n",
    "        q_values = []\n",
    "        prob = []\n",
    "        \n",
    "        for a in env.actions:\n",
    "            actions.append(a)\n",
    "            q_values.append(Q[state,a])   \n",
    "        for i in range(len(q_values)):\n",
    "            if i == np.argmax(q_values):\n",
    "                prob.append(1)\n",
    "            else:\n",
    "                prob.append(0)       \n",
    "                \n",
    "        pi[state] = (actions, prob)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(env, q, state):\n",
    "    actions = env.actions\n",
    "    action_values = []\n",
    "    for action in actions:\n",
    "        action_values.append(q[state, action])\n",
    "    return actions[np.argmax(action_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(env, e, q, state):\n",
    "    actions = env.actions\n",
    "    action_values = []\n",
    "    prob = []\n",
    "    for action in actions:\n",
    "        action_values.append(q[(state, action)])\n",
    "    for i in range(len(action_values)):\n",
    "        if i == np.argmax(action_values):\n",
    "            prob.append(1 - e + e/len(action_values))\n",
    "        else:\n",
    "            prob.append(e/len(action_values))\n",
    "    return actions, prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e-greedy policy is an extension of e_greedy()\n",
    "def generate_e_greedy_policy(env, e, Q):\n",
    "    pi = dict()\n",
    "    for state in env.states:\n",
    "        pi[state] = e_greedy(env, e, Q, state)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming ideal sample model that is the same as the environment\n",
    "def tabular_DynaQ(env, epsilon, alpha, num_iter, planning_steps):\n",
    "    Q = state_action_value(env)\n",
    "    pi = generate_e_greedy_policy(env, epsilon, Q)\n",
    "    for _ in range(num_iter):\n",
    "        current_state = np.random.choice(env.states)\n",
    "        model={}\n",
    "        d={}\n",
    "        while current_state != 0:\n",
    "            current_state = np.random.choice(env.states)\n",
    "            current_action = np.random.choice(pi[current_state][0], p = pi[current_state][1])\n",
    "            next_state, reward = env.state_transition(current_state, current_action)\n",
    "            if next_state==0:\n",
    "                break\n",
    "            best_action = greedy(env, Q, next_state)\n",
    "            Q[current_state, current_action] += alpha * (reward + env.gamma * Q[next_state, best_action] \\\n",
    "                                                         - Q[current_state, current_action])\n",
    "            model[current_state, current_action]= (next_state, reward)\n",
    "            \n",
    "            if current_state not in d:\n",
    "                d[current_state]=[current_action]\n",
    "            else:\n",
    "                if current_action not in d[current_state]:\n",
    "                    d[current_state].append(current_action)\n",
    "                    \n",
    "            for n in range(planning_steps):\n",
    "                state=random.choice(list(d.keys()))\n",
    "                action=random.choice(d[state])\n",
    "                new_state, new_reward =model[state, action]\n",
    "                new_best_action = greedy(env, Q, new_state)\n",
    "                Q[state, action] += alpha*(new_reward + env.gamma * Q[new_state, new_best_action] - Q[state, action])   \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = tabular_DynaQ(gw, 0.2 ,0.1, 1000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_hat = generate_greedy_policy(gw, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_policy(pi, env):\n",
    "    temp = np.zeros(len(env.states) + 2)\n",
    "    for s in env.states:\n",
    "        a = pi_hat[s][0][np.argmax(pi_hat[s][1])]\n",
    "        if a == \"U\":\n",
    "            temp[s] = 0.25\n",
    "        elif a == \"D\":\n",
    "            temp[s] = 0.5\n",
    "        elif a == \"R\":\n",
    "            temp[s] = 0.75\n",
    "        else:\n",
    "            temp[s] = 1.0\n",
    "            \n",
    "    temp = temp.reshape(4,4)\n",
    "    ax = seaborn.heatmap(temp, cmap = \"prism\", linecolor=\"#282828\", cbar = False, linewidths = 0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### RED = TERMINAL (0)\n",
    "### GREEN = LEFT\n",
    "### BLUE = UP\n",
    "### PURPLE = RIGHT\n",
    "### ORANGE = DOWN\n",
    "\n",
    "show_policy(pi_hat, gw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
